% Appendix Template

\chapter{SpaMHMM -- supplementary material} % Main appendix title

\label{appendix:spamhmm} % Change X to a consecutive letter; for referencing this appendix elsewhere, use \ref{AppendixX}

\section{Derivation of the EM learning algorithms for MHMM and SpaMHMM}

\subsection{EM for MHMM (\Algref{alg:mhmm})}
\label{sec:proof_em_noreg}
\Algref{alg:mhmm} follows straightforwardly from applying EM to the model defined by \twoeqrefs{eq:spamhmm_density}{eq:spamhmm_hmm} with the objective \plaineqref{eq:spamhmm_log_likelihood}. As explained in \secref{sec:background_em}, for table CPDs, the E-step consists in finding expected sufficient statistics $M(x, u)$ and $M(u)$, as defined in \twoeqrefs{eq:?}{eq:?} for each $(x, u) \in \mathrm{Val}(\rx,  \parents_\gG(\rx))$, where $\gG$ is the Bayesian network in \Figref{fig:spamhmm_bayesian_net}. Thus, in our setting, those equations yield:
\begingroup
\allowdisplaybreaks
\begin{align}
	&M(z,y) = \sum_{i=1}^{n} p(y, z \mid \vx_i, y_i) = \sum_{i=1}^{n} p(z \mid \vx_i, y_i) \1_{y=y_i}, \\
	&M(y) = \sum_{i=1}^{n} p(y \mid \vx_i, y_i) = \sum_{i=1}^{n} \1_{y=y_i}, \\
	&M(\rh^{(0)}=h, z) = \sum_{i=1}^{n} p(\rh^{(0)}=h, z \mid \vx_i, y_i) = \sum_{i=1}^{n} p(\rh^{(0)}=h \mid \vx_i, y_i, z) p(z \mid \vx_i, y_i) \nonumber\\
	&\hphantom{M(\rh^{(0)}=h,z)} = \sum_{i=1}^{n} p(\rh^{(0)}=h \mid \vx_i, y_i) p(z \mid \vx_i, y_i), \\
	&M(z) = \sum_{i=1}^{n} p(z \mid \vx_i, y_i), \\
	&M(\rh^{(t)}=h', \rh^{(t-1)}=h, z) = \sum_{i=1}^{n} p(\rh^{(t)}=h', \rh^{(t-1)}=h, z \mid \vx_i, y_i) \nonumber\\
	&\hphantom{M(\rh^{(t-1)}=h, } = \sum_{i=1}^{n} p(\rh^{(t)}=h' \mid \rh^{(t-1)}=h, \vx_i, z) p(\rh^{(t-1)}=h \mid \vx_i, z) p(z \mid \vx_i, y_i), \\
	&M(\rh^{(t-1)}=h, z) = \sum_{i=1}^{n} p(\rh^{(t-1)}=h, z \mid \vx_i, y_i) \nonumber\\
	&\hphantom{M(\rh^{(t-1)}=h, z)} = \sum_{i=1}^{n} p(\rh^{(t-1)}=h \mid \vx_i, z) p(z \mid \vx_i, y_i).
\end{align}
\endgroup
These equations allow us to compute the following updates in the M-step:
\begin{align}
	\evalpha^{(y)}_z = \frac{M(y,z)}{M(y)} \quad \text{and} \quad \evpi^{(z)}_h = \frac{M(\rh^{(0)}=h, z)}{M(z)}.
\end{align}
For the state transition matrices update, the same idea applies after summing over the sequence length:
\begin{equation}
	\emA^{(z)}_{h,h'} = \frac{\sum_{t=1}^T M(\rh^{(t)}=h', \rh^{(t-1)}=h, z)}{\sum_{t=1}^T M(\rh^{(t-1)}=h, z)}.
\end{equation}
Now, defining $n_y$, $\eta^{(z)}_i$, $\gamma^{(z)}_{i,h}(t)$, and $\xi^{(z)}_{i,h,h'}(t)$ as in \Algref{alg:mhmm}, the update formulas for $\evalpha^{(y)}_z$, $\evpi^{(z)}_h$, and $\emA^{(z)}_{h,h'}$ become as defined in that algorithm.

Deriving the update equations for the means and covariances of the Gaussian emission distributions is not so simple, since these are not table CPDs and therefore the procedure we have just used is no longer valid. Nonetheless, the desired equations can be obtained by explicitly maximizing the ELBO with respect to these parameters. As we have seen in \secref{sec:background_em},
\begin{equation}
	\mathrm{ELBO} = \sum_{i=1}^{n} \E_{(\rvh_i,\rz_i) \sim q} \log p(\mX_i, \rvh_i, \rz_i \mid y_i; \Theta),
\end{equation}
where $q(\rvh_i,\rz_i) = p(\rvh_i, \rz_i \mid \vx_i, y_i; \Theta^{(-)})$, being $\Theta^{(-)}$ the set of current values for all parameters. We are solely interested in maximizing the ELBO with respect to the parameters $\vmu^{(z)}_h$ and $\vsigma^{(z)}_h$, so we can plug in the expression for the joint $p(\vx_i, \rvh_i, \rz_i \mid y_i; \Theta)$ and ignore all terms that do not depend on these parameters:
\begin{align}
	\mathrm{ELBO} &= \sum_{i=1}^n \sum_{\rvh_i, \rz_i} q(\rvh_i,\rz_i) \log \left[p(\rz_i \mid y_i) p(\rh^{(0)}_i \mid \rz_i) \prod_t p(\rh^{(t)}_i \mid \rh^{(t-1)}_i, \rz_i) p(\vx^{(t)}_i \mid \rh^{(t)}_i, \rz_i) \right] \nonumber\\
	&= \sum_{i=1}^n \sum_{\rh^{(t)}_i, \rz_i} \sum_{t=1}^{T_i} q(\rh^{(t)}_i, \rz_i) \log p(\vx^{(t)}_i \mid \rh^{(t)}_i, \rz_i) + \text{const}.
\end{align}
Now, all that remains is computing the gradient of the ELBO with respect to the parameters and solving for the critical points:
\begin{align}
&\nabla_{\vmu^{(z)}_h} \mathrm{ELBO} = \sum_{i=1}^n \sum_{t=1}^{T_i} \frac{q(\rh^{(t)}_i=h, z)}{p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z)} \nabla_{\vmu^{(z)}_h} p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z) \nonumber\\
&\hphantom{\nabla_{\vmu^{(z)}_h} \mathrm{ELBO}} = \sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) \frac{\vx^{(t)}_i - \vmu^{(z)}_h}{{\vsigma^{(z)}_h}^2}, \\
&\nabla_{\vsigma^{(z)}_h} \mathrm{ELBO} = \sum_{i=1}^n \sum_{t=1}^{T_i} \frac{q(\rh^{(t)}_i=h, z)}{p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z)} \nabla_{\vsigma^{(z)}_h} p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z) \nonumber\\
&\hphantom{\nabla_{\vsigma^{(z)}_h} \mathrm{ELBO}} = \sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) \left(\frac{(\vx^{(t)}_i - \vmu^{(z)}_h)^2}{{\vsigma^{(z)}_h}^3} - \frac{1}{\vsigma^{(z)}_h}\right),
\end{align}
where vector division and exponentiation should be interpreted as elementwise operations. Finally, solving for the critical points yields:
\begin{align}
&\vmu^{(z)}_h = \frac{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) \vx^{(t)}_i}{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z)} \nonumber\\
&\hphantom{\vmu^{(z)}_h} = \frac{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)}) \vx^{(t)}_i}{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)})}, \\
&{\vsigma^{(z)}_h}^2 = \frac{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) (\vx^{(t)}_i - \vmu^{(z)}_h)^2}{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z)} \nonumber\\
&\hphantom{{\vsigma^{(z)}_h}^2} = \frac{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)}) (\vx^{(t)}_i - \vmu^{(z)}_h)^2}{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)})}.
\end{align}
Again, the formulas in \Algref{alg:mhmm} follow by plugging $\gamma^{(z)}_{i,h}(t)$ and $\xi^{(z)}_{i,h,h'}(t)$ into the expressions above. $\quad \square$

\subsection{EM for SpaMHMM \Algref{alg:mhmm}}
\label{sec:proof_em_reg}
Using the same notation as in \Secref{sec:proof_em_noreg}, we may rewrite \eqref{objective} as:
\begin{align}
J_r(\Theta) = &\frac{1}{N}\log \sum_{\rvz,\rmH} p(\tX,\rvz,\rmH \mid \vy,\Theta) \nonumber\\
& +\frac{\reg}{2} \sum_{j,k\neq j} \emG_{j,k} \E_{\rz \sim p(\rz \mid \ry=j, \Theta)} [ p(\rz \mid \ry=k, \Theta) ].
\end{align}
Despite the regularization term, we may still lower bound this objective by introducing a variational distribution $q(\rvz,\rmH)$ and using Jensen's inequality in the usual way:
\begin{align}
J_r(\Theta) &\geq \frac{1}{N} \E_{\rvz,\rmH \sim q} \left[\log \frac{p(\tX,\rvz,\rmH \mid \vy,\Theta)}{q(\rvz,\rmH)} \right] \nonumber\\
&\mathbin{\hphantom{=}}{} + \frac{\reg}{2} \sum_{j,k\neq j} \emG_{j,k} \E_{\rz \sim p(\rz \mid \ry=j, \Theta)} [ p(\rz \mid \ry=k, \Theta) ] \nonumber\\
&\coloneqq V_r(\Theta, q).
\end{align}
Clearly,
\begin{align}
\lefteqn{J_r(\Theta) - V_r(\Theta, q) =}\nonumber\\
&=\frac{1}{N} \left(\log p(\tX|\vy,\Theta) - \E_{\rvz,\rmH \sim q} \left[\log \frac{p(\tX,\rvz,\rmH \mid \vy,\Theta)}{q(\rvz,\rmH)} \right] \right) \nonumber\\
&= \frac{1}{N} \KL \left(q(\rvz,\rmH) || p(\rvz,\rmH \mid \tX,\vy,\Theta)  \right),
\end{align}
which, fixing the parameters $\Theta$ to some value $\Theta^{(-)}$ and minimizing with respect to $q$, yields the usual solution $q^*(\rvz,\rmH) = p(\rvz,\rmH \mid \tX,\vy,\Theta^{(-)})$. Thus, in the M-step, we want to find:
\begin{align}
\lefteqn{\argmax_\Theta V_r(\Theta, q^*) =} \nonumber\\
& =\argmax_\Theta \frac{1}{N}\sum_{\rvz,\rmH} p(\tX,\rvz,\rmH \mid \vy,\Theta^{(-)}) \log p(\tX,\rvz,\rmH \mid \vy,\Theta) \nonumber\\
& \mathbin{\hphantom{=}}{}+ \frac{\reg}{2} p(\tX|\vy,\Theta^{(-)}) \sum_{j,k\neq j} \emG_{j,k} \E_{\rz \sim p(\rz \mid \ry=j, \Theta)} [ p(\rz \mid \ry=k, \Theta) ] \nonumber\\
& = \argmax_\Theta \frac{1}{N}\tilde{J}(\Theta, \Theta^{(-)}) + \reg R(\Theta, \Theta^{(-)}) \nonumber\\
& \coloneqq \argmax_\Theta \tilde{J}_r(\Theta, \Theta^{(-)}),
\end{align}
where $\tilde{J}(\Theta, \Theta^{(-)})$ is as defined in \eqref{m_step_obj_noreg} and $R(\Theta, \Theta^{(-)})$ is our regularization (weighted by the data likelihood), which is simply a function of the parameters $\valpha_1,...,\valpha_K$:
\begin{align}
R(\Theta, \Theta^{(-)}) &= \frac{1}{2}p(\tX|\vy,\Theta^{(-)}) \sum_{j,k\neq j} \emG_{j,k} \E_{\rz \sim p(\rz \mid \ry=j, \Theta)} [ p(\rz \mid \ry=k, \Theta) ] \nonumber\\
&= \frac{1}{2}p(\tX|\vy,\Theta^{(-)}) \sum_{j,k\neq j} \emG_{j,k} \valpha_j \transp \valpha_k \nonumber\\
&= R(\valpha_1,...,\valpha_K,\Theta^{(-)}).
\end{align}
Now, we may build the Lagrangian as done in \Secref{sec:proof_em_noreg}. Since $R$ only depends on the $\valpha$'s, the update equations for the remaining parameters are unchanged. However, for the $\valpha$'s, it is not possible to obtain a closed form update equation. Thus, we use the reparameterization defined in \eqref{normalization} and update the new unconstrained parameters $\vbeta$ via gradient ascent. \\
We have:
\begin{align}
& \frac{\partial \tilde{J}}{\partial \alpha_{k,m}} = \frac{p(\tX|\vy,\Theta^{(-)})}{\alpha_{k,m}} \sum_i p(\rz_i=m \mid \mX_i,y_i,\Theta^{(-)}) \1_{y_i=k} \label{dJ_dalpha}, \\
& \frac{\partial R}{\partial \alpha_{k,m}} = p(\tX|\vy,\Theta^{(-)}) \sum_{j \neq k} \emG_{j,k} \alpha_{j,m}. \label{dR_dalpha}
\end{align}
From \twoeqrefs{dJ_dalpha}{dR_dalpha}, we see that the the resulting gradient $\nabla_{\boldsymbol{\alpha}_k} \tilde{J}_r = \frac{1}{N}\nabla_{\boldsymbol{\alpha}_k} \tilde{J} + \reg \nabla_{\boldsymbol{\alpha}_k} R$ is equal to some vector scaled by the joint data likelihood $p(\tX|\vy,\Theta^{(-)})$, which we discard since it only affects the learning rate, besides being usually very small and somewhat costly to compute. This option is equivalent to using a learning rate that changes at each iteration of the outter loop of the algorithm. \\
\Eqref{normalization} yields the following derivatives:
\begin{align}
& \frac{\partial \alpha_{k,m}}{\partial \beta_{k,m}} = \1_{\beta_{k,m} > 0} \frac{2\sigma'(\beta_{k,m})}{\sigma(\beta_{k,m})} \alpha_{k,m} (1 - \alpha_{k,m}), \\
& \frac{\partial \alpha_{k,m}}{\partial \beta_{k,l}} = \1_{\beta_{k,m} > 0} \frac{-2\sigma'(\beta_{k,m})}{\sigma(\beta_{k,m})} \alpha_{k,m} \alpha_{k,l}, \text{ for } l \neq m.
\end{align}
Finally, by the chain rule, we obtain:
\begin{align}
\lefteqn{\frac{\partial \tilde{J}}{\partial \beta_{k,m}} = \sum_l \frac{\partial \tilde{J}}{\partial \alpha_{k,l}} \frac{\partial \alpha_{k,l}}{\partial \beta_{k,m}}} \nonumber\\
&= \1_{\beta_{k,m} > 0} \frac{2\sigma'(\beta_{k,m})}{\sigma(\beta_{k,m})} \sum_i  \left(p(\rz_i=m \mid \mX_i,y_i,\Theta^{(-)}) -\alpha_{k,m}\right)\1_{y_i=k}, \\
\lefteqn{\frac{\partial R}{\partial \beta_{k,m}} = \sum_l \frac{\partial R}{\partial \alpha_{k,l}} \frac{\partial \alpha_{k,l}}{\partial \beta_{k,m}}} \nonumber\\
&= \1_{\beta_{k,m} > 0} \frac{2\sigma'(\beta_{k,m})}{\sigma(\beta_{k,m})} \alpha_{k,m}\sum_{j \neq k} \emG_{j,k}\left(\alpha_{j,m} - \valpha_j \transp \valpha_k \right).
\end{align}
Defining $\delta_{k,m} \coloneqq \frac{\partial \tilde{J}_r}{\partial \beta_{k,m}} = \frac{1}{N}\frac{\partial \tilde{J}}{\partial \beta_{k,m}} + \reg \frac{\partial R}{\partial \beta_{k,m}}$ and applying the gradient ascent update formula to $\beta_{k,m}$ the result follows.

\section{Getting the posterior distribution of observations in SpaMHMM}
\label{sec:posterior_proof}
In this section, we show how to obtain the posterior distribution $p(\rmX \mid \mX_\text{pref}, y)$ of sequences $\rmX \triangleq \left(\rvx^{(1)}, ...,\rvx^{(t)}\right)$ given an observed prefix sequence $\mX_\text{pref} \triangleq \left(\vx^{(-t_\text{pref}+1)}, ...,\vx^{(0)}\right)$, both coming from the graph node $y$. We start by writing this posterior as a marginalization with respect to the latent variable $\rz$:
\begin{align}
\label{mhmm_posterior}
p(\rmX \mid \mX_\text{pref}, y) &= \sum_{\rz} p(\rmX, \rz \mid \mX_\text{pref}, y)\nonumber\\
&= \sum_{\rz} p(\rmX \mid \mX_\text{pref}, y, \rz) p(\rz \mid \mX_\text{pref}, y) \nonumber\\
&= \sum_{\rz} p(\rmX \mid \mX_\text{pref}, \rz) p(\rz \mid \mX_\text{pref}, y),
\end{align}
where the last equality follows from the fact that the observations $\rmX$ are conditionally independent of the graph node $\ry$ given the latent variable $\rz$. The posterior $p(\rz \mid \mX_\text{pref}, y)$ may be obtained as done in \Algref{alg:mhmm}:
\begin{equation}
p(\rz \mid \mX_\text{pref}, y) \propto p(\mX_\text{pref} \mid z)p(z \mid y).
\end{equation}
We now focus on the computation of $p(\rmX \mid \mX_\text{pref}, \rz)$. Let $\rvh_\text{pref} \triangleq \left(\rh^{(-t_\text{pref}+1)}, ...,\rh^{(-1)}\right)$ and $\rvh \triangleq \left(\rh^{(0)}, ...,\rh^{(t)}\right)$, then:
\begin{align}
p(\rmX \mid \mX_\text{pref}, \rz) &= \sum_{\rvh_\text{pref}, \rvh} p(\rmX, \rvh_\text{pref}, \rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh_\text{pref}, \rvh} p(\rmX \mid \rvh_\text{pref}, \rvh, \mX_\text{pref}, \rz) p(\rvh_\text{pref}, \rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh_\text{pref}, \rvh} p(\rmX \mid \rvh, \rz) p(\rvh_\text{pref} \mid \rvh, \mX_\text{pref}, \rz) p(\rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh} p(\rmX \mid \rvh, \rz) p(\rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh} p(\rh^{(0)} \mid \mX_\text{pref}, \rz) \prod_{\tau=1}^{t} p(\rh^{(\tau)} \mid \rh^{(\tau-1)}, \rz) p(\rvx^{(\tau)} \mid \rh^{(\tau)}, z),
\end{align}
where we have used the independence assumptions that characterize the HMM. Here, the initial state posteriors $p(\rh^{(0)} \mid \mX_\text{pref}, \rz)$ are actually the final state posteriors for the sequence $\mX_\text{pref}$ for each HMM in the mixture, so they can also be computed as indicated \Algref{alg:mhmm}.

Thus, we see that, in order to obtain the posterior $p(\rmX \mid \mX_\text{pref}, y)$, we only need to update the mixture coefficients $p(\rz \mid \mX_\text{pref}, y)$ and the initial state probabilities $p(\rh^{(0)} \mid \rz, \mX_\text{pref})$. All remaining parameters are unchanged.