% Appendix Template

\chapter{SpaMHMM -- supplementary material} % Main appendix title

\label{appendix:spamhmm} % Change X to a consecutive letter; for referencing this appendix elsewhere, use \ref{AppendixX}

\section{Derivation of the EM learning algorithms for MHMM and SpaMHMM}

\subsection{EM for MHMM (\Algref{alg:mhmm})}
\label{sec:proof_em_noreg}
\Algref{alg:mhmm} follows straightforwardly from applying EM to the model defined by \twoeqrefs{eq:spamhmm_density}{eq:spamhmm_hmm} with the objective \plaineqref{eq:spamhmm_log_likelihood}. As mentioned in \Secref{sec:em_table_cpd}, for table CPDs, the E-step consists in finding expected sufficient statistics $M(w,\vv)$ and $M(\vv)$, as defined in \twoeqrefs{eq:joint_suf_stat}{eq:marginal_suf_stat}, for each $(w,\vv) \in \mathrm{Val}(\rw,\parents_\gG(\rw))$ and for each $\rw$ in $\gG$. In this case, $\gG$ is the Bayesian network in \Figref{fig:spamhmm_bayesian_net}. Thus, in our setting, those equations yield:
\begingroup
\allowdisplaybreaks
\begin{align}
	&M(z,y) = \sum_{i=1}^{n} p(y, z \mid \vx_i, y_i) = \sum_{i=1}^{n} p(z \mid \vx_i, y_i) \1_{y_i=y}, \\
	&M(y) = \sum_{i=1}^{n} p(y \mid \vx_i, y_i) = \sum_{i=1}^{n} \1_{y_i=y}, \\
	&M(\rh^{(0)}=h, z) = \sum_{i=1}^{n} p(\rh^{(0)}=h, z \mid \vx_i, y_i) = \sum_{i=1}^{n} p(\rh^{(0)}=h \mid \vx_i, y_i, z) p(z \mid \vx_i, y_i) \nonumber\\
	&\hphantom{M(\rh^{(0)}=h,z)} = \sum_{i=1}^{n} p(\rh^{(0)}=h \mid \vx_i, y_i) p(z \mid \vx_i, y_i), \\
	&M(z) = \sum_{i=1}^{n} p(z \mid \vx_i, y_i), \\
	&M(\rh^{(t)}=h', \rh^{(t-1)}=h, z) = \sum_{i=1}^{n} p(\rh^{(t)}=h', \rh^{(t-1)}=h, z \mid \vx_i, y_i) \nonumber\\
	&\hphantom{M(\rh^{(t-1)}=h, } = \sum_{i=1}^{n} p(\rh^{(t)}=h' \mid \rh^{(t-1)}=h, \vx_i, z) p(\rh^{(t-1)}=h \mid \vx_i, z) p(z \mid \vx_i, y_i), \\
	&M(\rh^{(t-1)}=h, z) = \sum_{i=1}^{n} p(\rh^{(t-1)}=h, z \mid \vx_i, y_i) \nonumber\\
	&\hphantom{M(\rh^{(t-1)}=h, z)} = \sum_{i=1}^{n} p(\rh^{(t-1)}=h \mid \vx_i, z) p(z \mid \vx_i, y_i).
\end{align}
\endgroup
These equations allow us to compute the following updates in the M-step:
\begin{align}
	\evalpha^{(y)}_z = \frac{M(y,z)}{M(y)} \quad \text{and} \quad \evpi^{(z)}_h = \frac{M(\rh^{(0)}=h, z)}{M(z)}.
\end{align}
For the state transition matrices update, the same idea applies after summing over the sequence length:
\begin{equation}
	\emA^{(z)}_{h,h'} = \frac{\sum_{t=1}^T M(\rh^{(t)}=h', \rh^{(t-1)}=h, z)}{\sum_{t=1}^T M(\rh^{(t-1)}=h, z)}.
\end{equation}
Now, defining $n_y$, $\eta^{(z)}_i$, $\gamma^{(z)}_{i,h}(t)$, and $\xi^{(z)}_{i,h,h'}(t)$ as in \Algref{alg:mhmm}, the update formulas for $\evalpha^{(y)}_z$, $\evpi^{(z)}_h$, and $\emA^{(z)}_{h,h'}$ become as defined in that algorithm.

Deriving the update equations for the means and covariances of the Gaussian emission distributions is not so simple, since these are not table CPDs and therefore the procedure we have just used is no longer valid. Nonetheless, the desired equations can be obtained by explicitly maximizing the ELBO with respect to these parameters. As we have seen in \Secref{sec:m_step},
\begin{equation}
	\label{eq:mhmm_elbo}
	\mathrm{ELBO} = \sum_{i=1}^{n} \E_{(\rvh_i,\rz_i) \sim q} \log p(\mX_i, \rvh_i, \rz_i \mid y_i; \Theta) + \text{const}
\end{equation}
where $q(\rvh_i,\rz_i) = p(\rvh_i, \rz_i \mid \vx_i, y_i; \Theta^{(-)})$, being $\Theta^{(-)}$ the set of current values for all parameters. We are solely interested in maximizing the ELBO with respect to the parameters $\vmu^{(z)}_h$ and $\vsigma^{(z)}_h$, so we can plug in the expression for the joint $p(\vx_i, \rvh_i, \rz_i \mid y_i; \Theta)$ and ignore all terms that do not depend on these parameters:
\begin{align}
	\label{eq:mhmm_elbo_gaussian_params}
	\mathrm{ELBO} &= \sum_{i=1}^n \sum_{\rvh_i, \rz_i} q(\rvh_i,\rz_i) \log \left[p(\rz_i \mid y_i) p(\rh^{(0)}_i \mid \rz_i) \prod_t p(\rh^{(t)}_i \mid \rh^{(t-1)}_i, \rz_i) p(\vx^{(t)}_i \mid \rh^{(t)}_i, \rz_i) \right] \nonumber\\
	&= \sum_{i=1}^n \sum_{\rh^{(t)}_i, \rz_i} \sum_{t=1}^{T_i} q(\rh^{(t)}_i, \rz_i) \log p(\vx^{(t)}_i \mid \rh^{(t)}_i, \rz_i) + \text{const}.
\end{align}
Now, all that remains is computing the gradient of the ELBO with respect to the parameters and solving for the critical points:
\begin{align}
&\nabla_{\vmu^{(z)}_h} \mathrm{ELBO} = \sum_{i=1}^n \sum_{t=1}^{T_i} \frac{q(\rh^{(t)}_i=h, z)}{p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z)} \nabla_{\vmu^{(z)}_h} p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z) \nonumber\\
&\hphantom{\nabla_{\vmu^{(z)}_h} \mathrm{ELBO}} = \sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) \frac{\vx^{(t)}_i - \vmu^{(z)}_h}{{\vsigma^{(z)}_h}^2}, \\
&\nabla_{\vsigma^{(z)}_h} \mathrm{ELBO} = \sum_{i=1}^n \sum_{t=1}^{T_i} \frac{q(\rh^{(t)}_i=h, z)}{p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z)} \nabla_{\vsigma^{(z)}_h} p(\vx^{(t)}_i \mid \rh^{(t)}_i=h, z) \nonumber\\
&\hphantom{\nabla_{\vsigma^{(z)}_h} \mathrm{ELBO}} = \sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) \left(\frac{(\vx^{(t)}_i - \vmu^{(z)}_h)^2}{{\vsigma^{(z)}_h}^3} - \frac{1}{\vsigma^{(z)}_h}\right),
\end{align}
where vector division and exponentiation should be interpreted as element-wise operations. Finally, solving for the critical points yields:
\begin{align}
&\vmu^{(z)}_h = \frac{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) \vx^{(t)}_i}{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z)} \nonumber\\
&\hphantom{\vmu^{(z)}_h} = \frac{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)}) \vx^{(t)}_i}{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)})}, \\
&{\vsigma^{(z)}_h}^2 = \frac{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z) (\vx^{(t)}_i - \vmu^{(z)}_h)^2}{\sum_{i=1}^n  \sum_{t=1}^{T_i} q(\rh^{(t)}_i=h, z)} \nonumber\\
&\hphantom{{\vsigma^{(z)}_h}^2} = \frac{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)}) (\vx^{(t)}_i - \vmu^{(z)}_h)^2}{\sum_{i=1}^n p(z \mid \mX_i, y_i; \Theta^{(-)}) \sum_{t=1}^{T_i} p(\rh^{(t)}=h \mid \mX_i, z; \Theta^{(-)})}.
\end{align}
Again, the formulas in \Algref{alg:mhmm} follow by plugging $\eta^{(z)}_i$ and $\gamma^{(z)}_{i,h}(t)$ into the expressions above. $\quad \square$

\subsection{EM for SpaMHMM (\Algref{alg:spamhmm})}
\label{sec:proof_em_reg}
We start by showing that the E-step is unaffected by the regularization term in \eqref{eq:spamhmm_objective} and then we will derive the update equations for the M-step.

As is a common practice in EM and variational methods, let $q(\rz, \rvh)$ be an arbitrary distribution over the hidden variables of our model whose support contains the support of $p$. Then,
\begin{align}
	J_r(\Theta) &= \frac{1}{n}\sum_{i=1}^n \log \E_{(\rz_i, \rvh_i) \sim q} \left[ \frac{p(\mX_i \mid y_i; \Theta)}{q(\rz_i, \rvh_i)} \right] + \reg R(\gG; \Theta) \nonumber\\
	&\geq  \frac{1}{n}\sum_{i=1}^n \E_{(\rz_i, \rvh_i) \sim q} \log \left[ \frac{p(\mX_i \mid y_i; \Theta)}{q(\rz_i, \rvh_i)} \right] + \reg R(\gG; \Theta).
\end{align}
where the inequality is a direct result of Jensen's inequality and $R(\gG; \Theta)$ is our regularization term, as defined in \eqref{eq:spamhmm_objective}. We are therefore interested in maximizing this lower bound with respect to both $\Theta$ and $q$. Since $R(\gG; \Theta)$ does not depend on $q$, maximization with respect to this distribution while fixing $\Theta = \Theta^{(-)}$ yields $q(\rz_i, \rvh_i) = p(\rz_i, \rvh_i \mid \mX_i, y_i; \Theta^{(-)})$, as for the case of standard (i.e. unregularized) EM. Thus,
\begin{equation}
	J_r(\Theta) \geq \frac{1}{n} \mathrm{ELBO}(\Theta) + \lambda R(\gG; \Theta) + \text{const},
\end{equation}
where the ELBO takes the exact same form as in \eqref{eq:mhmm_elbo}.

Now all that remains is the maximization of this lower bound with respect to $\Theta$. After noting that some parameters are constrained to sum up to 1, we could build the following Lagrangian and solve for the critical points:
\begin{align}
	L(\Theta, \Lambda) = & \frac{1}{n} \mathrm{ELBO}(\Theta) + \lambda R(\gG; \Theta) + \sum_{y=1}^k \mu_y\left(1 - ||\valpha^{(y)}||_{1}\right) \nonumber\\
	&+ \sum_{z=1}^m \sum_{h=1}^s \nu^{(z)}_h \left(1 - \sum_{h'=1}^s \emA^{(z)}_{h,h'}\right)
	+ \sum_{z=1}^m \varrho_z \left(1 - ||\vpi_z||_{1}\right),
\end{align}
where $\Lambda \triangleq \bigcup_{h,y,z} \{\mu_y, \nu^{(z)}_h, \varrho_z\}$ is the set of all Lagrange multipliers. Since $R(\gG; \Theta)$ only depends on the mixture coefficients $\valpha^{(y)}$, the update equations for all parameters except those are exactly the same as in \Algref{alg:mhmm}.

Unfortunately, setting $\nabla L = 0$ yields a system of equations that is non-linear in $\valpha^{(y)}$ and no analytical solution can be found. However, we can resort to the reparameterization defined in \eqref{eq:spamhmm_normalization} and update the new parameters $\vbeta^{(y)}$ by performing gradient ascent over the unconstrained lower bound. Thus, computing the required gradient concludes the derivation of the algorithm.

Now we only care about the dependency of the ELBO on the mixture coefficients, so, following the same idea as in \eqref{eq:mhmm_elbo_gaussian_params}, we can write:
\begin{align}
	\mathrm{ELBO} &= \frac{1}{n}\sum_{i=1}^n \sum_{\rz_i} q(\rz_i) \log p(\rz_i \mid y_i) + \text{const} \nonumber\\
	&= \frac{1}{n}\sum_{i=1}^n \sum_{z=1}^m q(z) \log \evalpha^{(y_i)}_{z} + \text{const},
\end{align}
and therefore,
\begin{equation}
	\frac{\partial \mathrm{ELBO}}{\partial \evalpha^{(y)}_{z}} = \frac{1}{n}\sum_{i=1}^n \frac{q(z)}{\evalpha^{(y)}_{z}} \1_{y_i=y}, \quad
	\frac{\partial R}{\partial \evalpha^{(y)}_{z}} = \frac{1}{2}\sum_{\substack{y'=1,\\y'\neq y}}^{k} \emG_{y,y'} \evalpha^{(y')}_{z}.
\end{equation}
Finally, by the chain rule,
\begin{align}
	&\frac{\partial \mathrm{ELBO}}{\partial \evbeta^{(y)}_{z}} = \sum_{z'=1}^{m} \frac{\partial \mathrm{ELBO}}{\partial \evalpha^{(y)}_{z'}} \frac{\partial \evalpha^{(y)}_{z'}}{\partial \evbeta^{(y)}_{z}} \nonumber\\
	&\hphantom{\frac{\partial \mathrm{ELBO}}{\partial \evbeta^{(y)}_{z}}} = \frac{1}{n} \sum_{i=1}^n \left(q(z) - \evalpha^{(y)}_{z}\right) \1_{y_i = y \wedge \evbeta^{(y)}_{z} > 0} \nonumber\\
	&\hphantom{\frac{\partial \mathrm{ELBO}}{\partial \evbeta^{(y)}_{z}}} = \frac{1}{n} \sum_{i=1}^n \left(p(z \mid \mX_i, y_i; \Theta^{(-)}) - \evalpha^{(y)}_{z}\right) \1_{y_i = y \wedge \evbeta^{(y)}_{z} > 0}, \\
	&\frac{\partial R}{\partial \evbeta^{(y)}_{z}} = \sum_{z'=1}^{m} \frac{\partial R}{\partial \evalpha^{(y)}_{z'}} \frac{\partial \evalpha^{(y)}_{z'}}{\partial \evbeta^{(y)}_{z}} \nonumber\\
	&\hphantom{\frac{\partial R}{\partial \evbeta^{(y)}_{z}}} = \evalpha^{(y)}_{z} \sum_{\substack{y'=1,\\y'\neq y}}^{k} \emG_{y,y'} \left(\evalpha^{(y')}_{z} - {\valpha^{(y')}} \transp \valpha^{(y)} \right) \1_{\evbeta^{(y)}_{z} > 0},
\end{align}
and considering $\psi^{(y)}_{z}$, $\omega^{(y)}_{z}$, and $\delta^{(y)}_z$ as defined in \Algref{alg:spamhmm} and the gradient ascent update rule the formulas follow. $\quad \square$

\section{Posterior distribution of observations}
\label{sec:posterior_proof}
In this section, we show how to obtain the posterior distribution $p(\rmX \mid \mX_\text{pref}, y)$ of sequences $\rmX \triangleq \left(\rvx^{(1)}, ...,\rvx^{(t)}\right)$ given an observed prefix sequence $\mX_\text{pref} \triangleq \left(\vx^{(-t_\text{pref}+1)}, ...,\vx^{(0)}\right)$, both coming from the graph node $y$. We start by writing this posterior as a marginalization with respect to the latent variable $\rz$:
\begin{align}
\label{mhmm_posterior}
p(\rmX \mid \mX_\text{pref}, y) &= \sum_{\rz} p(\rmX, \rz \mid \mX_\text{pref}, y)\nonumber\\
&= \sum_{\rz} p(\rmX \mid \mX_\text{pref}, y, \rz) p(\rz \mid \mX_\text{pref}, y) \nonumber\\
&= \sum_{\rz} p(\rmX \mid \mX_\text{pref}, \rz) p(\rz \mid \mX_\text{pref}, y),
\end{align}
where the last equality follows from the fact that the observations $\rmX$ are conditionally independent of the graph node $\ry$ given the latent variable $\rz$. The posterior $p(\rz \mid \mX_\text{pref}, y)$ may be obtained as done in \Algref{alg:mhmm}:
\begin{equation}
p(\rz \mid \mX_\text{pref}, y) \propto p(\mX_\text{pref} \mid \rz)p(\rz \mid y).
\end{equation}
We now focus on the computation of $p(\rmX \mid \mX_\text{pref}, \rz)$. Let $\rvh_\text{pref} \triangleq \left(\rh^{(-t_\text{pref}+1)}, ...,\rh^{(-1)}\right)$ and $\rvh \triangleq \left(\rh^{(0)}, ...,\rh^{(t)}\right)$, then:
\begin{align}
p(\rmX \mid \mX_\text{pref}, \rz) &= \sum_{\rvh_\text{pref}, \rvh} p(\rmX, \rvh_\text{pref}, \rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh_\text{pref}, \rvh} p(\rmX \mid \rvh_\text{pref}, \rvh, \mX_\text{pref}, \rz) p(\rvh_\text{pref}, \rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh_\text{pref}, \rvh} p(\rmX \mid \rvh, \rz) p(\rvh_\text{pref} \mid \rvh, \mX_\text{pref}, \rz) p(\rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh} p(\rmX \mid \rvh, \rz) p(\rvh \mid \mX_\text{pref}, \rz) \nonumber\\
&= \sum_{\rvh} p(\rh^{(0)} \mid \mX_\text{pref}, \rz) \prod_{\tau=1}^{t} p(\rh^{(\tau)} \mid \rh^{(\tau-1)}, \rz) p(\rvx^{(\tau)} \mid \rh^{(\tau)}, \rz),
\end{align}
where we have used the independence assumptions that characterize the HMM. Here, the initial state posteriors $p(\rh^{(0)} \mid \mX_\text{pref}, \rz)$ are actually the final state posteriors for the sequence $\mX_\text{pref}$ for each HMM in the mixture, so they can be computed as in \Algref{alg:forward_backward}.

Thus, we conclude that inference in the posterior model $p(\rmX \mid \mX_\text{pref}, y)$ corresponds to inference in a (Spa)MHMM where the original mixture coefficients and initial state probabilites are replaced with their respective posteriors, $p(\rz \mid \mX_\text{pref}, y)$ and $p(\rh^{(0)} \mid \mX_\text{pref}, \rz)$. All remaining parameters are unchanged.