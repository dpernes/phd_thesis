
%-------------------------------------------------------------------------
%	QUOTATION PAGE
%-------------------------------------------------------------------------
\quotepage{Matt Smith as \emph{The Doctor}, written by Matthew Graham}
{
	I am and always will be the optimist, the hoper of far-flung hopes and the
	dreamer of \newline improbable dreams
}

%-------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS PAGE
%-------------------------------------------------------------------------
\begin{acknowledgements}

Acknowledge ALL the people!

\end{acknowledgements}

\addvspacetoc{0.3cm} % Add a gap in the Contents, for aesthetics


%-------------------------------------------------------------------------
%	ABSTRACT PAGE
%-------------------------------------------------------------------------
\begin{abstract}

Most machine learning algorithms, and particularly those conceived for supervised classification, assume that the data they will be trained on are independent and identically distributed (i.i.d.). This assumption ignores the fact that, in many circumstances, the available training data is a collection of subdatasets, each one being originated at a distinct data source and hence being sampled from its own distribution. Furthermore, the data observed at test time often suffers from the same distribution shift problem, again violating the i.i.d.\ assumption.

In this thesis, we address the problem of learning from multiple data distributions under various settings. First, we consider the case where multiple data sources are spread over a given medium and produce sequential data streams. We show that, despite the data distributions for each of the sources are different, inter-source correlations can be exploited to learn a better-performing model for the generative distribution of each source.

Afterwards, we focus on out-of-distribution generalization, i.e.\ to the setting where the test distribution is unknown or only partially known at training time. Initially, we consider the problem of multi-source domain adaptation. Here, annotated data from multiple data sources is combined to learn a classification model for a fixed target distribution, for which no labeled data is available at training time. We research several possible deep neural network architectures for the problem of object counting in videos from multiple cameras and we propose a novel algorithm for multi-source domain adaptation for non-sequential data that achieves state-of-the-art results.

Finally, we address domain generalization. This problem differs from domain adaptation in the fact that the target domain is not known, and therefore no data from this distribution is available at training time, neither labeled nor unlabeled. Sign language recognition is the main application considered here, since a truly useful automatic sign language recognition system should be able to perform accurately regardless of the signer that is using it. We propose two algorithms to learn a signer-independent sign classifier. The first one is an improvement over an existing method based on adversarial neural networks and the second and best-performing one uses a variational autoencoder to learn highly discriminative signer-invariant representations.

\end{abstract}

%-------------------------------------------------------------------------
%	ABSTRACT PAGE (PORTUGUESE)
%-------------------------------------------------------------------------
\begin{abstract}[
	thesistitle={Aprendizagem a partir de dados multi-entidade},
	title={Resumo},
	degree={Doutoramento em Ciência de Computadores},
	nameconnector={por}]
\begin{otherlanguage}{portuguese}

Este tese é sobre alguma coisa

\end{otherlanguage}
\end{abstract}

%-------------------------------------------------------------------------
%	LIST OF CONTENTS/FIGURES/TABLES
%-------------------------------------------------------------------------

\addvspacetoc{0.3cm}
\setcounter{tocdepth}{3}
\tableofcontents % Write out the Table of Contents

\listoffigures % Write out the List of Figures

%\listoftables % Write out the List of Tables

%\addvspacetoc{0.3cm}

%-------------------------------------------------------------------------
%	PHYSICAL CONSTANTS/OTHER DEFINITIONS
%-------------------------------------------------------------------------

%\begin{listofcontants}
%	\const{My little ponny test of magical rainbow}{$mn/mp$}
%    {$2.997\ 924\ 58\times10^{8}\ \mbox{ms}^{-\mbox{s}}$}
%   \const{Vaccuum permeability test of magical rainbow for a specific case of
%   condensed matter physics}
%   {$\epsilon_0$}{$2.997\ 924\ 58\times10^{8}\ \mbox{ms}^{-\mbox{s}}$}
%	\const{Speed of Light test of magical rainbow}{$c$}
%    {$2.997\ 924\ 58\times10^{8}\ \mbox{ms}^{-\mbox{s}}$}
%\end{listofcontants}


%-------------------------------------------------------------------------
%	SYMBOLS
%-------------------------------------------------------------------------

%\begin{listofsymbols}
%	\symb{$F_{\mu\nu}$}{Maxwell tensor}{F}
%	\symb{$a$}{distance}{m}
%	\\
%	\symb{$\omega$}{angular frequency}{rads$^{-1}$}
%\end{listofsymbols}


%-------------------------------------------------------------------------
%	NOTATION
%-------------------------------------------------------------------------

\newcommand\notationname{Notation and Conventions}
\addtotoc{\notationname}
\fancyhead[LO]{\textsc{\notationname}}

\input{Notation}



%-------------------------------------------------------------------------
%	ABBREVIATIONS
%-------------------------------------------------------------------------

%\begin{glossary}
%	\abbrev{QM}{Quantum Mechanics}
%\end{glossary}


%-------------------------------------------------------------------------
%	DEDICATORY
%-------------------------------------------------------------------------

%\begin{dedicatory}
%	This thesis is dedicated to my family, for their love and support.
%\end{dedicatory}
