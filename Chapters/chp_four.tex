% Chapter Template

% Main chapter title
%\chapter[toc version]{doc version}
\chapter{Domain generalization}

% Short version of the title for the header
%\chaptermark{version for header}

% Chapter Label
% For referencing this chapter elsewhere, use \ref{ChapterTemplate}
\label{chp:domain_generalization}

% Write text in here
% Use \subsection and \subsubsection to organize text

\begin{tcolorbox}
	\small{
		Some parts of this chapter were originally published in or adapted from:
		\begin{itemize}
			\item[] \cite{DeSIRe} \bibentry{DeSIRe} (presented in \Secref{sec:desire})
			\item[] \cite{AdvSInvConf} \bibentry{AdvSInvConf} (presented in \Secref{sec:adv_signer_inv})
			\item[] \cite{AdvSInvJournal} \bibentry{AdvSInvJournal} (idem)
			\item[] \cite{AdvInvAttack} \bibentry{AdvInvAttack} (\Secref{sec:adv_inv_attack})
		\end{itemize}

		The first two authors contributed equally in \cite{DeSIRe} and \cite{AdvSInvConf}. Both conceived the models and designed and conducted the experiments, with the supervision of Rebelo and Cardoso. The work in \cite{AdvSInvJournal} extends \cite{AdvSInvConf} by including a more exhaustive experimental evaluation. In \cite{AdvInvAttack}, Diogo Pernes contributed on the development of the proposed methodology, together with the first two authors, who formalized the problem and conducted all experiments. Cardoso supervised the work.
	}
\end{tcolorbox}

\section{Introduction}
\label{sec:chp4_intro}
In Chapters \ref{chp:networked_data_streams} and \ref{chp:domain_adaptation}, the target entities/domains were known at training time. In Chapter \ref{chp:networked_data_streams}, we exploited the correlations between different but related entities to augment the amount of data available for each of those and hence improve the in-distribution generalization. Chapter \ref{chp:domain_adaptation} was dedicated to the problem of domain adaptation, whose purpose is to improve the out-of-distribution (OOD) generalization in a specific target domain for which no labeled data is available.

In this chapter, we shall continue focusing on OOD generalization. However, now, the target domain is unknown and, therefore, no data from this domain is available at training time, neither labeled nor unlabeled. The purpose, then, is to use labeled data from multiple source domains to build a discriminative model that generalizes well to unknown OOD target domains -- a problem known as \emph{domain generalization} (\citet{Blanchard2011, Muandet2013}). Our main assumption to accomplish this goal is that the set of features that are relevant for the learning task are domain-invariant. Formally, we assume that, for each domain $\gD$, there exists a bijection $b_\gD: \gX \mapsto \gZ \times \gW$, where $\gZ$ is the domain-invariant space of features used for classification and $\gW$ are domain-specific auxiliary features carrying no relevant signal for the considered learning task. Thus, for $(\rz, \rw) \triangleq b_\gD(\rvx)$, we assume that $p_\gD(\ry \mid \rx) = p(\ry \mid \rz)$, i.e.\ the optimal classifier for any domain $\gD$ can be reconstructed from features in $\gZ$ and a domain-invariant classifier $p(\ry \mid \rz)$.  This formulation is closely related to the covariate shift assumption for domain adaptation, described in \secref{sec:cov_shift_sota}.

A computer vision application where this problem is particularly relevant is sign language recognition (SLR). Large inter-signer variability in the manual signing process of sign languages is one of the challenges associated with this task. Due to this issue, models trained on data from a given set of signers often fail to generalize well when tested on previously unseen signers. Since, ideally, an SLR system should be able to recognize the gestures of any signer, this problem should be tackled with domain generalization (DG) techniques. For this reason, SLR will be the main application considered in this chapter. Nonetheless, we will also show that the same principles can be applied successfully to develop a fingerprint presentation attack detection method that exhibits robust performance on detecting unseen attacks.

The remainder of this chapter is organized as follows: i) we start by presenting the state of the art for DG (\secref{sec:dg_sota}); ii) we present a novel adversarial-based approach for DG in the context of SLR (\secref{sec:adv_signer_inv}); iii) we show how this methodology can be successfully adapted to address the problem of iris presentation attack detection (\secref{sec:adv_iris_attack}); iv) we present a novel reconstruction-based algorithm for DG (\secref{sec:desire}).

\section{State of the art}
\label{sec:dg_sota}
\citet{Zhou2021} divide the algorithms for domain generalization as heterogeneous and homogeneous, depending on whether the label space varies (heterogeneous DG) or not (homogeneous DG). The former case is also known as \emph{zero-shot} domain generalization and its goal is in general to learn a feature representation that can be used in the target domain to recognize new classes. The latter, which will be the focus of this chapter, is closely related to domain adaptation, so there is a significant intersection between the two. \citet{Albuquerque2019} presented an upper bound for the generalization error that is essentially an upper bound for multi-source domain adaptation, similar to the bound by \citet{Zhao2018} (Theorem~\ref{thm:da_bound_multi_source}) and to our own (Theorem~\ref{thm:target_risk_bound}).

The theoretical proximity between the two problems motivates the existence of similar algorithms to tackle them. As a matter of fact, many algorithms for DG follow the paradigm of domain alignment, which we have discussed extensively in the context of DA. \citet{Li2018} use an adversarial autoencoder and maximum mean discrepancy over its latent space to learn domain-invariant features. \citet{Ghifary2015} address the same problem through a multi-output autoencoder, which is trained to transform samples from one domain into samples from the remaining domains with the same label. \citet{Motiian2017} proposed a unified framework to address the problems of domain adaptation and generalization. They use a contrastive $\normltwo$-loss in the latent space that pushes together samples from different domains and the same class while pulling apart samples from different classes. Several other approaches extend the idea of domain adversarial networks (\citet{Ganin2015}) to the problem of domain generalization, by using domain classifiers and minimax training to learn domain-invariant features. Some of those use a single multi-class classifier to classify samples into one of $k$ source domains (e.g.\ \citet{Aslani2020, Matsuura2020}) and others employ $k$ binary domain discriminators trained in a one-vs-all manner (e.g.\ \citet{Shao2019, YaLi2018}).

Ensemble learning has also been widely applied to the problem of domain generalization. \citet{Zheng2014} train support vector machines (SVMs) with a single positive example and a few negative examples (known as \emph{exemplar-SVMs}) and use the most confident classifiers in an ensemble to make the final prediction. More recent approaches replace the SVM with deep neural networks and build ensembles of domain-specific networks, either by weighting all the predictions equally (e.g.\ \citet{Innocente2018, Zhou2020}) or by using the output of a domain classifier as sample-dependent ensemble weights (\citet{Wang2020a}).

Self-supervised learning (SSL) techniques are becoming increasingly popular in machine learning and have also been applied to the problem of DG. SSL refers to the task of learning from free labels, i.e.\ it consists of standard supervised learning for tasks where the labels can be extracted automatically from the data, without the need of manual annotation. Examples of SSL tasks are predicting the next word in a sentence, image colorization (\citet{Zhang2016}), predicting the relative position of image patches (\citet{Doersch2015}), predicting if a video is being played forward or backward (\citet{Wei2018}), etc. The idea motivating SSL is that the features learned by pretraining the model on self-supervised tasks provide good initializations for the model, which can then be finetuned for the desired task using a smaller amount of annotated data. In the scope of DG, SSL provides useful features regardless of the target task, reducing the overfitting to domain-specific biases (\citet{Zhou2021}). This idea was followed by \citet{Carlucci2019} and \citet{Wang2020b}, who trained a network to solve the Jigsaw puzzle (i.e.\ to place nine shuffled image patches back into their correct positions) as an auxiliary task to enhance domain generalization.

For a more complete review of DG theory and algorithms, please see \citet{Wang2021} and \citet{Zhou2021}.

\section{Adversarial domain generalization for signer-independent sign language recognition}
\label{sec:adv_signer_inv}

\subsection{Introduction}
Sign language is an integral form of communication and, currently, considered the standard education method of deaf people worldwide. It is a visual means of communication, with its own lexicon and grammar, that combines articulated hand gestures along with facial expressions to convey meaning. Deaf people have difficulty in speaking and learning spoken languages like hearing people. However, with sign language, they are able to communicate as efficiently and seamlessly. The population of sign language speakers is extended to family and friends of the deaf, interpreters and the curious, who learn the language by their own initiative. As most hearing people are unfamiliar with sign language, deaf people find it difficult to interact with the hearing majority. The result is the isolation of deaf communities from the overall society.

In this regard, automatically analyzing and recognizing sign language has become one of the key problems in the human computer-interaction field. SLR systems are meant to automatically translate signs into the corresponding text or speech. This is important not only to bridge the communication gap between deaf and hearing people but also to increase the amount of content the deaf can access, such as the creation of educational tools or games for deaf people and visual dictionaries of sign language.

The SLR problem has been addressed in the literature by means of wearable devices (e.g.\ data gloves or similar equipments) or vision-based systems (\citet{Ahdal2012}). Vision-based systems, either those using color or depth information, face the problem of the inherently noisy and ambiguous nature of the input data. Data gloves yield more reliable and descriptive features. Nevertheless, vision-based SLR systems are arguably the most natural choice for real-world applications. Vision-based SLR is less invasive since there is no need to wear cumbersome devices that may affect the natural signing movement.

Several vision-based SLR methodologies have been proposed over the last twenty years, with increasing progress in the recognition performance. An important part of this recent progress was achieved thanks to the emergence of deep learning approaches and more specifically with CNNs (\citet{Pigou2015, Koller2016, Wu2016, Neverova2016, Kumar2017}).

A practical SLR system must operate in a signer-independent scenario. That is, the signer of the probe must not be seen during the training process of the models. Although current SLR systems demonstrate excellent performance for signer-dependent settings, their recognition rates typically decrease significantly when the signer is new to the system. This performance drop is the result of the large inter-signer variability in the manual signing process of sign languages.

Although the appearance of manual signs is well-defined in sign language dictionaries, in practice, variations may arise due to regional and social factors, and also from age, gender, education and family background. This can lead to significant variations in manual signs performed by different signers, and pose challenging problems for developing robust signer-independent SLR systems. \Figref{fig:inter_signer_variations} illustrates inter-signer variability by showing six different signers performing the same gestures.

\begin{figure}[t]
    \centering
    \subfloat{\includegraphics[width=0.15\textwidth]{ChapterFour/1.png}}
    \hfill
    \subfloat{\includegraphics[width=0.15\textwidth]{ChapterFour/4.png}}
    \hfill
    \subfloat{\includegraphics[width=0.15\textwidth]{ChapterFour/3.png}}
    \\\vspace{0.4cm}
    \subfloat{\includegraphics[width=0.15\textwidth]{ChapterFour/2.png}}
    \hfill
    \subfloat{\includegraphics[width=0.15\textwidth]{ChapterFour/5.png}}
    \hfill
    \subfloat{\includegraphics[width=0.15\textwidth]{ChapterFour/6.png}}
    \caption{Inter-signer variability: it is possible to observe not only phonological variations (e.g.\ different handshapes, palm orientations, and sign locations) but also a large physical variability (e.g.\ different hand sizes) when six signers are performing the same sign.}
    \label{fig:inter_signer_variations}
\end{figure}

Borrowing from recent works on adversarial neural networks (\citet{Goodfellow2014, Feutry2018}) and domain transfer (\citet{Ganin2015}), we introduce a deep neural network along with a novel adversarial training objective to specifically tackle the signer-independent SLR problem. The underlying idea is to preserve as much information as possible about the signs, while discarding the signer-specific information that is implicitly present in the manual signing process. For this purpose, the proposed deep model is composed by an \emph{encoder} network, which maps from the input images to latent representations, as well as two discriminative classifiers operating on top of these underlying representations, namely the \emph{sign-classifier} network and the \emph{signer-classifier} network. While the former is trained to predict the sign labels, the latter is trained to identify the signer. In addition, the parameters of the encoder network are optimized to minimize the loss of the sign-classifier while trying to fool the signer-classifier network. This adversarial and competitive training scheme encourages the learned representations to be signer-invariant and highly discriminative for the sign classification task. To further constrain the latent representations to be signer-invariant, we introduce an additional training objective that operates on the hidden representations of the encoder network in order to enforce the latent distributions of different signers to be as similar as possible.

Although this adversarial training framework is similar to those initially introduced by \citet{Ganin2015}, in the context of domain adaptation, and then by \citet{Feutry2018} to learn anonymized representations, our main contributions on top of these works are two-fold: i) the application of the adversarial training concept to the signer-independent SLR problem and ii) a novel adversarial training objective that differs from the ones of \citet{Ganin2015} and \citet{Feutry2018} in two ways. First, our training objective is minimum if and only if the adversarial classifier (i.e.\ the signer-classifier) produces a uniform distribution over the domains (i.e.\ signer identities). Second, we introduce an additional term to the adversarial training objective that further discourages the learned representations of retaining any signer-specific information, by explicitly imposing similarity in the latent distributions of different signers.

The remainder of this section is organized as follows. \Secref{sec:adv_signer_inv_rel_work} presents the related work on SLR. The proposed model along with its adversarial training scheme are fully described in \secref{sec:adv_signer_inv_method}. Experimental results and conclusions are reported in sections~\ref{sec:adv_signer_inv_experiments} and \ref{sec:adv_signer_inv_conclusion}, respectively.

\subsection{Related Work}
\label{sec:adv_signer_inv_rel_work}
We have discussed some of the most relevant approaches for DG in \secref{sec:dg_sota}, so now we shall focus our attention on the specific problem of SLR. This has become an appealing topic in modern society because such systems can ideally be used to reduce the communication barriers that exist between deaf and hearing people.
SLR approaches can be broadly divided into: (i) isolated, which address the recognition of single signs either using static images or video (\citet{Marin2014, Marin2016}), and (ii) continuous, which correspond to the recognition of sentences represented as a sequence of signs (\citet{DanGuo2017, DanGuo2018, Wang2018}). Although most recent works focus on the continuous SLR and its associated problems (e.g.\ large vocabulary size), static SLR is still a challenging task, especially under unconstrained scenarios. One of the biggest challenges is related to the large inter-signer variability, which is in fact the focus of this work.

According to the amount of data required from the test signers, previously signer-independent SLR works can be broadly divided into two main groups: (i) signer adaptation approaches, where a previous trained model is adapted to a new signer by using a small amount of signer specific data, and (ii) truly signer independent methodologies, in which a generic model robust for new test signers is built without using data of those test signers.

The former signer adaptation approaches were greatly inspired by speaker adaptation methods from the speech recognition research. \citet{Agris2006} used maximum likelihood linear regression (MLLR) and maximum a posteriori (MAP) estimation for signer adaptation. In a subsequent work (\citet{Agris2008a}), they extended their work by combining the eigenvoice approach by \citet{Kuhn2000} with MLLR and MAP to adapt trained hidden Markov models (HMMs) to new signers. MLLR and MAP were the basic adaptation strategies, and the eigenvoice approach provided constraints to reduce the number of free parameters to be adapted. More recently, \citet{Kim2016} investigated the potential of several signer normalization techniques (e.g.\ speed normalization) and different deep neural network adaptation strategies for the signer-independence problem. They found that while signer normalization is ineffective, a simple neural network adaptation strategy, such as fine-tuning the signer-specific neural networks on the adaptation data, is very effective.

The aforementioned methods are all supervised adaptation approaches, in the sense that the adaptation data from the new signer must be labeled. However, in practice, collecting labeled data may be a cumbersome and time-consuming task. To overcome this issue, a few works have resorted to unsupervised adaptation strategies. \citet{Yin2015} proposed a two-step weakly supervised metric learning framework to perform signer adaptation with some unlabeled sign data of the new signer. In the first step, a generic metric is learnt from the available labeled data of several different signers. In the second step, the generic metric is adapted to the new signer by considering clustering and manifold constraints along with the collected unlabeled data.

Although signer adaptation is a reasonable approach, there is still the need to collect either labeled or unlabeled data to retrain and adapt the model to a new signer. Therefore, a truly signer-independent approach, which does not require any data from the new signers, would be the ideal solution for a practical SLR system. Examples of such works can be found are those by \citet{Zieren2005, Shanableh2011, Agris2008b, Kong2014, Kelly2010, Dahmani2014, Yin2016}. Most of them involved a huge feature engineering effort in order to build normalized feature descriptors robust to the physical variations of the signers (e.g.\ height, hand size and length of the arm) and different acquisition conditions (e.g.\ distance to the camera). Afterwards, most of these works use HMMs or their variants for sign recognition. It is the example of the work proposed by \citet{Agris2008b}, in which a set of 11 regional features are extracted (e.g.\ 2-D coordinates, hand blobs area, orientation of the main axis, inertia ratio, eccentricity and compactness) and then normalized according to the head position and shoulders distance of the signer. \citet{Kelly2010} introduced a novel signer-independent hand posture feature descriptor, along with an eigenspace size function which represents both qualitative and quantitative properties of a visual shape. \citet{Kong2014} gave particular importance to the movement of epenthesis (ME), which appears as the transition movement that connects successive signs. Concretely speaking, they removed the ME by using a segment and merge approach to decrease the inter-signer variations in ME and used a two-layer conditional random field classifier for sign recognition. More recently, \citet{Yin2016} proposed an interesting and alternative approach that relies on distance metric learning. In particular, the metric is learnt by constraining the distances between the training samples and generic references of the sign classes. The references are constructed by signer invariant representations of each sign class (i.e.\ the average of all samples within the specific class). Afterwards, a two-step iterative optimization strategy is employed to obtain more appropriate references and update the corresponding distance metric alternately.

Although the aforementioned methods have promoted a significant evolution in the signer-independent research, there are still many opportunities for improvement. A major weakness across all the methods is related to the fact that representation and metric learning are not performed jointly. It is well known that the recent success of deep learning approaches, particularly those using CNNs, in tasks like object detection and recognition, has been extended to the SLR problem. The underlying motivation is to automatically learn multiple levels of representations directly from the data (\citet{Pigou2015, Koller2016, Wu2016, Neverova2016, Kumar2017}). However, none of these explicitly constrains the learned representations to be signer invariant.

\section{Methodology}
\label{sec:adv_signer_inv_method}

The ultimate goal of our model is to learn signer-invariant latent representations that preserve the relevant part of the information about the signs while discarding the signer-specific traits that may hamper the sign classification task. To accomplish this purpose, we introduce a deep neural network along with an adversarial training scheme that is able to learn feature representations that combine both sign discriminativeness and signer-invariance.

More specifically, let $\{(\mX_{i},y_{i},s_{i})\}_{i=1}^{n}$ be a labeled dataset of $n$ samples, where $\mX_{i}$ represents the $i$-th colour image, and $y_{i} \in \gY$ and $s_{i} \in \{1,2,\dots,k\}$ denote the corresponding class (sign) label and signer identity, respectively. To induce the model to learn signer-invariant representations, the proposed model comprises three distinct sub-networks:
\begin{itemize}
    \item an encoder network, which aims at learning an encoding function $g(\cdot;\vtheta_g): \gX \mapsto \gZ$, parameterized by $\vtheta_g$, that maps from an input image $\mX \in \gX$ to a latent representation $\vz \in \gZ$;
    \item a sign-classifier network, which operates on top of this underlying latent representation $\vz$ to learn our task-specific function $h(\cdot; \vtheta_h): \gZ \mapsto [0,1]^{|\gY|}$, parameterized by $\vtheta_h$, that maps latent vectors into the predicted probabilities of each sign class;
    \item a signer-classifier network, with the purpose of learning a signer-specific function $d(\cdot; \vtheta_d): \gZ \mapsto [0,1]^k$, parameterized by $\vtheta_d$, that maps the same hidden representation $\vz$ into the predicted probabilities of each signer identity.
\end{itemize}

During the learning stage, the parameters of both classifiers are optimized in order to minimize their errors on their specific tasks on the training set. In addition, the parameters of the encoder network are optimized in order to minimize the loss of the sign-classifier network while forcing the signer-classifier to be a random guessing predictor. In the course of this adversarial training procedure, the learned latent representations $\vz$ are encouraged to be signer-invariant and highly discriminative for sign classification. To further discourage the latent representations of retaining any signer-specific traits, we introduce an additional training objective that enforces the latent distributions of different signers to be as similar as possible.

\subsubsection{Architecture}
As illustrated in \Figref{fig:model_archi}, the architecture of the proposed model is composed by three main sub-networks or blocks, i.e. an encoder, a sign-classifier, and a signer-classifier.

The encoder network attempts to learn a mapping from an input image $\mX$ to a latent representation $\vz$. It consists of a sequence of three pairs of consecutive $3\times 3$ convolutional layers with Rectified  Linear  Units (ReLUs) as non-linearities. For downsampling, the last convolutional layer of each pair has a stride of 2. The number of filters starts as 32 and is doubled after each convolutional pair. The dense layer on top of the encoder network has 128 neurons. On top of that, there is a fully-connected layer, also with a ReLU, outputting the desired signer-invariant latent representations $\vz$.

Taking the latent representations $\vz$ as input, the sign-classifier block is composed by a sequence of three fully-connected layers, with ReLUs as the non-linear functions, for predicting the sign class $\hat{y} \triangleq \argmax h(\vz; \vtheta_h)$. The number of nodes of each hidden layer was set to 128. The last fully-connected layer has a softmax activation function which outputs the probabilities for each sign class.

The signer-classifier network has exactly the same topology as the sign-classifier. However, it maps the latent representations $\vz$ to the predicted signer identity $\hat{s} \triangleq \argmax d(\vz; \vtheta_d)$. Therefore, the number of nodes of the output layer equals the number of signers in the training set.

\input{Figures/ChapterFour/fig_adv_signer_inv.tex}

\subsubsection{Adversarial training}
By definition, signer-invariant representations discard all signer-specific information and, as such, no function (i.e.\ classifier) exists that maps such representations into the correct signer identity. This naturally leads to an adversarial problem, in which: (i) a signer-classifier network $d(\cdot; \vtheta_d)$ receives latent representations $\vz \triangleq g(\mX;\vtheta_g)$ from an encoder network $g(\cdot;\vtheta_g)$ and tries to predict the signer identity $s$ corresponding to image $\mX$ and (ii) the encoder network tries to fool the signer-classifier network while still providing good representations for the sign-classifier network $h(\cdot; \vtheta_h)$, which in turn receives the same representations $\vz$ and aims to predict the sign label $y$ corresponding to image $\mX$.

Therefore, the signer-classifier network shall be trained to minimize the negative log-likelihood of correct signer predictions:
\begin{equation}
\label{eq:signer_loss}
\min_{\vtheta_d} \; \left\lbrace L_{\text{signer}}(\vtheta_g, \vtheta_d) \triangleq -\frac{1}{n}\sum_{i=1}^n \log p(s_i \mid g(\mX_{i}; \vtheta_g); \theta_d) \right\rbrace
\end{equation}

In the perspective of the encoder, the predictions of the sign-classifier should be as accurate as possible and the predictions of the signer-classifier should be kept close to uniform, meaning that this latter model is not capable of doing better than random guessing the signer identity. Formally, this may be translated into the following constrained objective:
\begin{align}
\label{eq:sign_loss}
&\min_{\vtheta_g, \vtheta_h} \; \left\lbrace L_{\text{sign}}(\vtheta_g, \vtheta_h)\triangleq-\frac{1}{n}\sum_{i=1}^n \log p(y_i \mid g(\mX_{i}; \vtheta_g); \vtheta_h)\right\rbrace,\\
\label{eq:kl_obj}
&\text{subject to } \; \frac{1}{n}\sum_{i=1}^n \KL(\gU(\rs) || p(\rs \mid g(\mX_{i}; \vtheta_h); \vtheta_g) \leq \epsilon,
\end{align}
where $\KL$ is the Kullback-Leibler (KL) divergence and $\gU(\rs)$ denotes the discrete uniform distribution on the random variable $\rs$, defined over the set of signer identities $\{1, 2, \dots, k\}$ in the training set. Here, $\epsilon \geq 0$ determines how far from uniform the signer-classifier predictions are allowed to be (as measured by the KL divergence). The choice of the uniform distribution implies the underlying assumption that the training set is balanced relatively to the number of examples per signer (which should be true for most practical datasets). When this is not the case, the empirical distribution of signer identities in the training set may be used instead.

The inequality constraint~\plaineqref{eq:kl_obj} may be rewritten as:
\begin{equation}
\label{eq:adv_loss}
L_{\text{adv}}(\vtheta_g, \vtheta_d) \triangleq \frac{1}{nk}\sum_{i=1}^n \sum_{\rs} \log p(\rs \mid g(\mX_{i}; \theta_g); \vtheta_d) \leq \epsilon + \log k,
\end{equation}
and the constrained optimization problem may be equivalently formulated as:
\begin{equation}
\label{eq:tr_obj}
\min_{\vtheta_g, \vtheta_h}  \left\lbrace L(\vtheta_g, \vtheta_h, \vtheta_d) \triangleq L_{\text{sign}}(\vtheta_g, \vtheta_h) + \mu_d L_{\text{adv}}(\theta_g, \theta_d) \right\rbrace,
\end{equation}
where $\mu_d \geq 0$ depends on $\epsilon$ and $L_{\text{adv}}$ plays the role of an adversarial loss with respect to the signer classification loss $L_{\text{signer}}$.

This objective and the structure of our model are similar to those used by \citet{Ganin2015}, in the context of domain adaptation, and by \citet{Feutry2018}, to learn anonymized representations for privacy purposes. However, the former uses the negative signer classification loss as the adversarial term (i.e.\ $L_{\text{adv}} \leftarrow -L_{\text{signer}}$), which is not lower bounded, leading to high gradients and more difficult optimization. The latter addresses this problem by replacing this term with the absolute difference between the adversarial loss as defined in \eqref{eq:adv_loss} and the signer classification loss (i.e.\ $L_{\text{adv}} \leftarrow |L_{\text{adv}} - L_{\text{signer}}|$). This option has a nice information theoretic interpretation as being an empirical upper-bound for the mutual information between the distribution of signer identities and the distribution of latent representations. Nonetheless, there exist infinitely many (non-uniform) distributions for which this loss vanishes. Our choice, besides being clearly lower bounded by the entropy of the uniform distribution, $\log k$, is minimum if and only if $p(\rs \mid g(\mX_{i}; \vtheta_g); \vtheta_d) \equiv \gU(\rs)$, $\forall i$, meaning that the signer-classifier block is completely agnostic relatively to the signer identities of the training samples.

\subsubsection{Signer-transfer training objective}
To further encourage the latent representations $\vz$ to be signer-invariant, we introduce an additional term in objective~\plaineqref{eq:tr_obj}, the so-called signer-transfer loss $L_{\text{transfer}}$. The core idea of $L_{\text{transfer}}$ is to match first order statistics of different signers earlier in the network. For this purpose, let $g^{(l)}(\cdot;\vtheta_g)$ be the $l$-th layer of the encoder network, $l \in \{1,...,m\}$, and consider the distance $\gD^{(l)}(s, s'; \vtheta_g)$ between two distinct signers $s$ and $s'$, defined as:
\begin{equation}
\label{eq:sign_transfer_pairwise_loss}
\gD^{(l)}(s, s'; \vtheta_g) \triangleq \Big|\Big| \frac{1}{n_s} \sum_{i=1}^n g^{(l)}(\mX_{i}; \vtheta_g) \1_{s_i = s} - \frac{1}{n_{s'}}\sum_{j=1}^n g^{(l)}(\mX_{j}; \vtheta_g) \1_{s_j = s'} \Big|\Big|_2^2,
\end{equation}
where $||\bcdot||_2$ is the $\normltwo$-norm, and $n_s$ and $n_{s'}$ denote the number of training examples for signers $s$ and $s'$, respectively. Accordingly, the signer-transfer loss at the $l$-th layer is the sum of the pairwise distances between all signers, i.e.:
\begin{equation}
L_{\text{transfer}}^{(l)}(\vtheta_g) \triangleq \sum_{\substack{s,s'=1 \\ s' \neq s}}^k \gD^{(l)}(s,s'; \vtheta_g).
\end{equation}
The overall signer-transfer loss $L_{\text{transfer}}$ is then a weighted sum of the losses computed at each layer of the encoder network:
\begin{equation}
\label{eq:signer_transfer_loss}
L_{\text{transfer}}(\vtheta_g) \triangleq \sum_{l=1}^{m} \beta^{(l)} L_{\text{transfer}}^{(l)}(\vtheta_g),
\end{equation}
where $\beta^{(l)}\geq 0$ is a hyperparameter that controls the relative importance of the loss obtained at the $l$-th layer.
By combining \plaineqref{eq:tr_obj} and \plaineqref{eq:signer_transfer_loss}, the encoder and sign-classifier networks are trained to minimize the following loss function:
\begin{equation}
\label{eq:enc_sign_loss}
\min_{\vtheta_g, \vtheta_h} \left\lbrace L(\vtheta_g, \vtheta_h, \vtheta_d) \triangleq L_{\text{sign}}(\vtheta_g, \vtheta_h) + \mu_d L_{\text{adv}}(\vtheta_g, \vtheta_d) +
\mu_t L_{\text{transfer}}(\vtheta_g) \right\rbrace,
\end{equation}
where $\mu_t \geq 0$ is the weight that controls the relative importance of the signer-transfer term.

Summing up, the adversarial training procedure is organized by alternating between the minimization of objective~\plaineqref{eq:enc_sign_loss} and the minimization of objective~\plaineqref{eq:signer_loss}.

\subsection{Experimental evaluation}
\label{sec:adv_signer_inv_experiments}

%\subsection{Datasets and evaluation protocol}
\hspace{\parindent}The experimental evaluation of the proposed model was performed using two publicly available SLR databases: the Jochen-Triesch database (\citet{Triesch2001}), and the Microsoft Kinect and Leap Motion American sign language (MKLM) database (\citet{Marin2014, Marin2016}).
Jochen-Triesch is a dataset of 10 hand signs performed by 24 signers against three different types of backgrounds: uniform light, uniform dark and complex. Experiments on this dataset were conducted using its standard evaluation protocol (\citet{Just2006}), in which 8 signers are used for training and the remaining 16 signers are used for the test. MKLM contains a total of 10 signs, each one repeated 10 times by 14 different signers. In this dataset, the performance of the models is assessed using 5 random splits, with disjoint sets of signers for training, validation, and testing, yielding at each split a training set of 10 signers, a validation set of 2 signers and a test set of 2 signers.

\subsubsection{Implementation details}
In order to extract the manual signs from the noisy background of the images, the automatic hand detection algorithm \cite{Ferreira2018} is used as a pre-processing step. The images are then cropped, resized to the average sign size of the training set, and normalized to be in the range $[-1,1]$.

Throughout this section, the proposed model is compared with state of the art methods for each dataset. Nevertheless, to further attest the robustness of the proposed model, two different baselines are also implemented:
\begin{itemize}
    \item (Baseline 1) A CNN trained from scratch with $\normltwo$ regularization. For a fair comparison, the architecture of the baseline CNN corresponds to the architecture of the encoder network followed by the sign-classifier network of the proposed model.
    \item (Baseline 2) A CNN with the baseline 1 topology, but trained with the triplet loss (\citet{Schroff2015}).
\end{itemize}
Here, the triplet loss concept is explored in order to impose signer-independence in the representation space and, hence, build up a more robust baseline. The underlying idea is to minimize the distance between an \textit{anchor} and a \textit{positive} latent representation, $\vz_{y_{i}, s_{i}}$ and $\vz_{y_{p}, s_{p}}$, respectively; while maximizing the distance between the anchor $\vz_{y_{i}, s_{i}}$ and a \textit{negative} representation $\vz_{y_{n}, s_{n}}$. It is important to note that while anchor and positive latent representations have to be from the same sign class, their signer identity may or not change. On the other hand, anchor and negative representations are from different sign classes, whereas their signer identity may also change. In order to train baseline 2 in an end-to-end fashion for sign classification, the overall loss function to be minimized is a trade-off between the triplet loss $L_{\text{triplet}}$, defined below, and the classification loss $L_{\text{sign}}$:
\begin{equation}
L_{\text{triplet}} \triangleq \frac{1}{n}\sum_{i=1}^{n}\Big[||\vz_{y_{i}, s_{i}}-\vz_{y_{p}, s_{p}}||^2_2 - ||\vz_{y_{i}, s_{i}}-\vz_{y_{n}, s_{n}}||^2_2 + \alpha\Big],
\end{equation}
where $y_{p}=y_{i}$ and $y_{n} \neq y_{i}$ and the margin $\alpha$ enforced between \emph{positive} and \emph{negative} pairs was fixed at $\alpha=1$. In addition, following \citet{Schroff2015}, we adopted an \emph{online} triplet generation strategy, by selecting the hardest positive/negative samples within every mini-batch. The overall loss for this model is therefore $L_{\text{sign}} + \rho L_{\text{triplet}}$, where $\rho \geq 0$ is a hyperparameter.

All deep models were implemented in PyTorch and trained with the Adam optimization algorithm using a batch size of 32 samples. For reproducibility purposes, the source code as well as the weights of the trained models are publicly available online\footnote{\url{https://github.com/pmmf/SI-SLR}}. The hyperparameters that are common to all the implemented models (i.e.\ learning rate and $\normltwo$ regularization weight) as well as some hyperparameters that are specific to the proposed model (i.e.\ $\mu_d$ and $\mu_t$) and to the implemented baseline~2 (i.e.\ $\rho$) were optimized by means of a grid search approach and cross-validation on the training set (see \Tableref{tab:hyperparam} for more details). The signer-transfer penalty $L_{\text{transfer}}$ is applied to the last two layers of the encoder network with a relative weight of 1.

\begin{table}[t]
    \centering
    \resizebox{0.5\columnwidth}{!}{
        \begin{tabular}{c|c|c}
            Hyperparameters                    & Symbol & Set                \\ \hline
            Leaning rate                                & --      & \{$1\text{e}^{-04}$,$1\text{e}^{-03}$\}             \\
            $\normltwo$-norm coefficient                              & --       & \{$1\text{e}^{-05}$,$1\text{e}^{-04}$\}             \\
            $L_{\text{triplet}}$ weight                 & $\rho$                & \{0.1,0.5,1,5,10\}                  \\
            $L_{\text{adv}}$ weight                 & $\mu_d$                & \{0.1,0.5,0.8,1,3\}                  \\
            $L_{\text{transfer}}$ weight                 & $\mu_t$                & \{$1.5\text{e}^{-04}$,$2\text{e}^{-04}$,$4\text{e}^{-04}$,$1\text{e}^{-03}$\}                  \\
        \end{tabular}
    }
    \caption{Hyperparameter sets.}
    \label{tab:hyperparam}
\end{table}

\subsubsection{Results and discussion}
Experiments on the Jochen-Triesch and MKLM databases are summarized in \Tableref{tab:joint_table}. We compare our method with other state of the art approaches that have published results on these datasets (\citet{Dahmani2014, Just2006, Kelly2010, Marin2014, Ferreira2018}). The results on the Jochen-Triesch database are presented in terms of average classification accuracy in the overall test set as well as against each specific background type (i.e.\ uniform and complex). For the MKLM database, \Tableref{tab:joint_table}~(B) shows the average classification accuracy computed across all test splits, as well as the minimum and maximum accuracy value achieved by each method.

\begin{table}[t]
    \centering
    \resizebox{1\textwidth}{!}{
        \begin{tabular}{cc}

            \begin{tabular}{c|c c c}
                & \multicolumn{3}{c}{Background}                \\
                & Uniform        & Complex         & Both          \\ \hline
                \citet{Just2006}       & 92.79           & 81.25           & 87.92         \\
                \citet{Kelly2010} & 91.80            & --               & --             \\
                \citet{Dahmani2014} & 93.10            & --               & --             \\ \hline
                CNN (Baseline 1)                            & 97.50           & 74.38           & 89.79         \\
                CNN with Triplet loss (Baseline 2)          & 98.13           & 75.63           & 90.63              \\
                Proposed method       &      \textbf{98.75}     & \textbf{ 91.25}
                &   \textbf{96.25}     \\
            \end{tabular}

            &

            \begin{tabular}{c|c c c}
                & Average (std)  & min         & max \\ \hline
                \citet{Marin2014}       & 89.71 ( -- )             & --                    & --            \\
                \citet{Ferreira2018} & 93.17 ( -- )             & --                    & --            \\\hline
                CNN (Baseline 1)                              & 89.90 (8.81)            & 73.00                & 98.00        \\
                CNN with Triplet loss (Baseline 2)            & 91.40 (3.93)            & 86.50                & 96.50        \\
                Proposed method        &    \textbf{94.80 (3.53)}      &   \textbf{89.50}
                &  \textbf{100.00}       \\
            \end{tabular} \\

            (A) Jochen-Triesch & (B) MKLM \\
    \end{tabular}}
    \caption{Classification accuracy (\%) on (A) Jochen-Triesch and (B) MKLM datasets.}
    \label{tab:joint_table}
\end{table}

The most relevant observation is the superior performance of the proposed model. Specifically, the proposed model provides the best overall classification accuracy on both SLR databases, clearly outperforming both implemented baselines and all the previous state of the art models. In complex scenarios, as reported in \Tableref{tab:joint_table}~(A), the proposed model surpasses all the other methods by a large margin. In addition, by analyzing the standard deviation as well as the minimum and maximum accuracy values, it is possible to observe that the proposed model is the method with the lowest variability, yielding consistently high accuracy rates across all test splits of the MKLM dataset (see \Tableref{tab:joint_table}~(B)). These results attest the robustness of the proposed model and its capability of better dealing with the large inter-signer variability that exists in the manual signing process of sign languages. Interestingly, the obtained results also reveal that the implemented baselines are in fact fairly strong models, both of them outperforming most of the state of the art methods on both datasets.

\Tableref{tab:loss_terms} illustrates the effect of each proposed training scheme by itself. For this purpose, the proposed model was trained either (i) with just the adversarial procedure, without the signer-transfer $L_{\text{transfer}}$ loss, or (ii) with just the $L_{\text{transfer}}$ penalty on the encoder network without adversarial training. The results clearly demonstrate the complementary effect between the two training procedures, as their combination provides the best overall classification accuracy. Interestingly, each training scheme outperforms on its own both baselines and state of the art methods.

\begin{table}[t]
    \centering
    \begin{small}
        \begin{tabular}{c|c c : c }
            & Adversarial ($L_{\text{adv}}$) only  & Signer-transfer ($L_{\text{transfer}}$) only & Both \\ \hline
            Jochen-Triesch                              & 95.21            & 94.38                & \textbf{96.25}        \\
            MKLM            & 94.00            & 94.10                & \textbf{94.80}        \\
        \end{tabular}
    \end{small}
    \caption{\centering The effect of each training procedure in the proposed model. The results in the last column are replicated from \Tableref{tab:joint_table} as they include both training procedures.}
    \label{tab:loss_terms}
\end{table}

\subsubsection{Latent space visualization}
To further demonstrate the effectiveness of the proposed model in promoting signer-invariant latent representation spaces, we show in \Figref{fig:adv_signer_inv_tsne} a visual inspection of the latent representations through the t-distributed stochastic neighbor embedding (t-SNE, \citet{Maaten2008}). These plots clearly demonstrate the better capability of the proposed model of imposing signer-independence in the latent representations. The proposed model yields a latent representation space in which representations of different signers and same class are close to each other and well mixed, while it keeps latent representations of different classes far apart. By analyzing the t-SNE plot of baseline 1, it is possible to observe that the latent representations of different signers and the same class tend to be far apart in the latent space. In addition, there is some overlapping between clusters of different classes. Although baseline 2 (CNN with the triplet loss) promoted slightly improvements over the standard baseline CNN, the proposed model achieved by far the best signer-invariance and class separability.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{ChapterFour/tsne_baseline.png}
        \caption{CNN -- baseline 1}
        \label{fig:adv_signer_inv_tsne_a}
    \end{subfigure}
    %\hfill%
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{ChapterFour/tsne_triplet.png}
        \caption{CNN with triplet loss -- baseline 2}
        \label{fig:adv_signer_inv_tsne_b}
    \end{subfigure}
    %\hfill%
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{ChapterFour/tsne_proposed.png}
        \caption{Proposed model}
        \label{fig:adv_signer_inv_tsne_c}
    \end{subfigure}
    \caption{\centering Two-dimensional projection of the latent representation space using the t-distributed stochastic neighbor embedding (t-SNE). Markers $\bullet$ and $\textbf{+}$ represent 2 different test signers, while the different colors denote the 10 sign classes.}
    \label{fig:adv_signer_inv_tsne}
\end{figure}

%-------------------------------------------------------------------------
\subsection{Conclusion}
\label{sec:adv_signer_inv_conclusion}

This paper presents a novel adversarial training objective, based on representation learning and deep neural networks, specifically designed to tackle the signer-independent SLR problem. The underlying idea is to learn signer-invariant latent representations that preserve as much information as possible about the signs, while discarding the signer-specific traits that are irrelevant for sign recognition. For this purpose, we introduce  an adversarial training procedure for simultaneously training an \textit{encoder} and a \textit{sign-classifier} over the target sign variables, while preventing the latent representations of the \textit{encoder} to be predictive of the signer identities. To further discourage the underlying representations of retaining any signer-specific information, we propose an additional training objective that enforces the latent distributions of different signers to be as similar as possible.
Experimental results demonstrate the effectiveness of the proposed model in several SLR databases.

\section{Adversarial domain generalization for iris presentation attack detection}
\label{sec:adv_iris_attack}
In this section, we show how our adversarial domain generalization model for signer-invariant SLR (presented in \Secref{sec:adv_signer_inv}) can be adapted to a specific biometrics-related application.

\subsection{Introduction}
\label{sec:adv_iris_attack_intro}
Biometric recognition systems are considered reliable enough to be deployed in government and civilian applications. The shift from controlled samples acquisition to a more autonomous one increased the vulnerabilities of these systems. Unfortunately, presentation attack detection (PAD) measures had not grown robustly along with this quick evolution and several weak points can be exploited when performing unsupervised biometric identification as such in mobile biometrics, for example. Successful spoofing attempts have been made public in a matter of days, or even hours, after the release of high-tech devices equipped with biometric recognition. The iris recognition sensor of Samsung S8 was reportedly spoofed by German researchers by simply printing a photo of the authorised user and placing a contact lens in it (\citet{cccsamsung2017}). More recently, the quick hack of Samsung Galaxy S10 ultrasonic fingerprint sensor suggests no presentation attack detection measures of any kind. It is fair to conclude that industry does not share the same enthusiasm as academic community on anti-spoofing measures denoted by the good amount of research continuously produced (\citet{raghavendra2015VSIA,czajka2018irisPADreview,Galbally2019,scherhag2019}).

Fortunately, exceptions are starting to show in commercial products, like the recent case of the Apple iPhone `Face ID'' case~\footnote{www.biometricupdate.com/201812/android-devices-facial-recognition-fooled-by-3d-printed-head-but-not-face-id} or the FaceTec ZoOm® technology (\citet{facetec102019}). Undoubtedly this change is motivated and supported by initiatives that encourage the development and `open testing' of spoofing coutermeasures such as `The National Voluntary Laboratory Accreditation Program'' (NVLAP) from NIST \footnote{The NVLAP provides third-party accreditation to testing and calibration laboratories in response to legislative actions or requests from government agencies or private-sector organizations. NVLAP-accredited laboratories are assessed against the management and technical requirements from ISO/IEC 17025:2017.}

Nevertheless, research-wise there are still open problems to address. Here, we focus on the fact that most PAD techniques are based on falsely optimistic evaluation methodologies (\citet{sequeira2016realistic}): traditionally, the classification models are designed and then evaluated using datasets comprising \emph{bona fide} presentations and a specific species of presentation attack instruments (PAI). The case when a PAI in the test set is significantly different from the ones used for training is overlooked. What if such sample has a higher probability to circumvent the system than the ones drawn from the original training dataset? To solve this research question it is necessary to develop robust methods to cope with sophisticated and unseen attacks as our eventual intruders become more capable and successfully develop new spoofing techniques.

The aforementioned problem has in fact been addressed before regarding iris, fingerprint and face (often targeted under the open-set or anomaly detection contexts). However, it still remains a challenging topic. Despite the importance of iris as a biometric trait for recognition purposes, in our view, the study of iris PAD generalization problem to unseen PAI species (PAIS) has not been yet fully studied in literature.

The remainder of this section is organized as follows: i) we start by summarizing the related work on the topic (\Secref{sec:adv_iris_attack_rel_work}); ii) we formalize the problem and emphasize the necessary modifications that had to be done to the model presented in \Secref{sec:adv_signer_inv} to adapt it to this new application (\Secref{sec:adv_iris_attack_method}); iii) we present experimental results that confirm the effectiveness of the model (\Secref{sec:adv_iris_attack_experiments}); iv) we conclude this section with some final remarks (\Secref{sec:adv_iris_attack_conclusion}).

\subsection{Related work}
\label{sec:adv_iris_attack_rel_work}

Recent PAD methods in general, and iris-focused ones in particular, have demonstrated remarkable performances. However, a methodological limitation can be pointed as it is recurrently found that these results are obtained when training and test data comprise the same type of attacks, i.e.\ the same PAIS. This problem has been addressed and proved that the performance rates of these PAD methods typically decrease significantly when the PAIS is new to the system (\citet{marasco2011robustness,bowyer2014cosmetic,sequeira2016realistic}). This performance drop may be result of the large inter-`PAI-species' variability. A practical PAD system must operate in a `PAI-species'-independent scenario, which means that the type of PAIS of the test set must not be seen during the training routine of the models. This problem is one of the crucial problems for the development of real-world PAD systems and it has frequently been tackled in literature as an open-set or anomaly detection problem.

The pioneer work that raised the evaluation of PAD methods across different types and unseen PAIS appeared in the fingerprint domain with the work of \citet{marasco2011robustness}. \citet{rattani2015openset} and \citet{sequeira2015fingerprint}, despite using different approaches, both relied on the idea of enforcing the knowledge of the bona fide presentations over the attacks to better deal with unseen PAIS. \citet{bowyer2014cosmetic} studied the evaluation of a binary classification on contact lenses iris spoofing attacks. By using an unseen type on the test set the authors showed that using the same lens types in both the training and testing data can give a very misleading idea of the accuracy of the method.

A step forward was made by combining methodologies designed for print and contact lenses attack (\citet{sequeira2014ildmma}). Eventually, the construction of a new database comprising several types of iris PAIS (\citet{raghavendra2015VSIA}) allowed new evaluation scenarios. \citet{sequeira2016realistic} state that whenever a new PAIS is presented in the test step, the performance of the classifier drops significantly and that an improvement can be obtained when a one-class classifier is trained only with bona fide presentations.

One-class classification was also used for face by \citet{kittler2017faceanomaly}. With the rise of deep learning (DL) techniques, PAD methods have been proposed applying deep representations for iris, face and fingerprint (\citet{menotti2015deep,pinto2018counteracting}), following the same binary approach. Recent works investigate the robustness of DL fingerprint PAD methods to deal with unseen PAI species (\citet{tolosana2018towards}).

Until recently, most of the proposed approaches, either assume overly optimistic assumptions about the attacker  (binary classification approaches) or only use part of the data (and therefore, of the knowledge) available at training time to design the models (one-class approaches). Therefore, the goal of this work is to present an iris PAD method that uses the information of both bona fide and available attack presentations and is robust to unseen PAI species. This objective will be achieved by enforcing the learning of the task of distinguishing the bona fide from the attack presentations while at the same time ensuring the invariance between the different type of the PAI species.

\subsection{Methodology}
\label{sec:adv_iris_attack_method}
The approach adopted here coincides in most aspects with the one described in \Secref{sec:adv_signer_inv}, so we shall focus on describing the slight differences that exist. Now, the data consists of $\{\mX^{(\text{bf})}_i\}_{i=1}^{n_{\text{bf}}} \cup \{(\mX^{(\text{a})}_i,s_i)\}_{i=1}^{n_{\text{a}}}$, i.e.\ there is one set containing $n_{\text{bf}}$ bona fide examples and another one containing $n_{\text{a}}$ attack examples. Each attack example $\mX^{(\text{a})}_i$ is annotated with the corresponding PAI species label $s_i \in \{1,2,\dots,k\}$.

In this problem, we are solely interested in classifying samples as bona fide or attack, thus $h(\cdot;\vtheta_h)$ (formerly designated as sign-classifier) is now a binary classifier. More importantly, we want to obtain latent representations that are invariant to the PAI species, but the latent representations of bona fide examples should be easily separable from these. Thus, now, only the attack samples are fed through adversarial classifier $d(\cdot;\vtheta_d)$ (formerly designated as signer-classifier) and used for the adversarial training routine. For the same reason, the transfer loss $L_{\text{transfer}}$ approximating first-order statistics only applies to these samples too. \Figref{fig:adv_iris_model} presents the model architecture and hopefully makes the differences between this and our previous model even clearer.

\input{Figures/ChapterFour/fig_adv_iris.tex}

\subsection{Experiments}
\label{sec:adv_iris_attack_experiments}
We use the Visible Spectrum Iris Artefact (VSIA) Database (\citet{raghavendra2015VSIA}) in our experiments. This dataset comprises five different presentations combining print and electronic screen attacks: i) Print Attack (PA); ii) iPad Electronic Screen Attack (ESA); iii) Samsung Galaxy Tab ESA; (iv) combined PA \& ESA using iPad; and v) combined PA \& ESA using Samsung Pad. The methods are evaluated by leaving out one PAI species for testing. The training set is therefore divided in one specie for validation and the remaining used for training. Also the same set of samples are used for testing across the different experiments to allow precise comparison of the results. Following \citet{sequeira2016realistic}, weighted local binary pattern features (wLBP, \citet{zhang2010contact}) were extracted in a preprocessing step and fed as input to the network, which in this case consists of an MLP.

Our model was compared to a baseline consisting of the same classifier without the PAI species classifier and adversarial training and to an SVM operating on top of the same wLBP features (\citet{sequeira2016realistic}). Results are in \Tableref{tab:pad_accuracy}. Comparing the accuracy for each attack, it can be observed that uniquely replacing the SVM with an MLP does not result in an improvement. This can be explained by the fact that the dataset has a very limited size and therefore the MLP method tends to overfit due to the lack of training samples. It was not for no reason that SVMs ruled for a long time in the pattern recognition domain. However, the proposed adversarial approach outperformed the SVM for most attacks and on average as well.

For further results and details about the experiments, please see \citet{AdvInvAttack}.

\begin{table}[t]
    \centering
    \resizebox{0.8\columnwidth}{!}{
        \begin{tabular}{c|c c c c c:c}
            & Attack i)
            & Attack ii)
            & Attack iii)
            & Attack iv)
            & Attack v)
            & Avg. \\ \hline

            wLBP+SVM~\cite{sequeira2016realistic}
            &78.85     &90.39     &\bf 98.08 &\bf 95.68 & 97.12     & 92.02 \\
            Baseline wLBP+MLP
            &78.00     &\bf 93.00  &94.50    &90.00   &95.50     &90.20 \\
            Proposed wLBP+$\text{MLP}_\text{adv}$
            &\bf82.00  &\bf 93.00  &98.00   &94.50   &\bf 97.50  &\bf 93.00 \\
    \end{tabular}}
    \caption{Presentation attack detection accuracy (\%) in the VSIA dataset.}
    \label{tab:pad_accuracy}
\end{table}

\section{Conclusions}
\label{sec:adv_iris_attack_conclusion}
This work proposed a method to improve the robustness and generalization capacity of an iris PAD method to new attacks. The goal of the proposed model is to learn latent representations invariant to the PAI species that preserve relevant information about the PAD properties while discarding the `PAI-species'-specific aspects that may hamper the PAD classification task. The proposed regularization strategies made the PAD method `PAI-species'-independent and robust to new test PAIS.
The experiments were based in comparing a baseline MLP and an MLP trained with adversarial strategies using as input highly discriminative features (wLBP) extracted from the images. When comparing the baseline MLP to an SVM classifier the results are quite similar or even worse. This can be explained simply by the fact that the dataset has a very limited size and the MLP method will overfit.
However, applying the adversarial regularization strategy significantly improved the PAD robustness of the method. The obtained results clearly suggest that the application of deep learning techniques with additional strategies will provide breakthroughs in this challenge.

\section{DeSIRe: Deep Signer-Invariant Representations for Sign Language Recognition}
\label{sec:desire}

\subsection{Introduction}
In \Secref{sec:adv_signer_inv}, we introduced a method for domain generalization which uses adversarial neural networks to align the marginal distributions of multiple source domains. The method showed promising results for both visual (\Secref{sec:adv_signer_inv}) and non-visual data (\Secref{sec:adv_iris_attack}). Here, we again focus our attention on vision problems and, specifically, on solving the problem of signer-independent SLR.

To specifically tackle the signer-independent SLR problem, we now present DeSIRe, a novel deep neural network that aims to learn \textbf{De}ep \textbf{S}igner-\textbf{I}nvariant \textbf{Re}presentations. The underlying idea is to explicitly enforce the model to automatically learn highly discriminative signer-invariant feature representations from the data by aligning and regularizing conditional distributions in a latent space. To accomplish this goal, the DeSIRe model consists of two main modules or components, namely a conditional variational autoencoder (CVAE) and a classifier. Specifically, the main task of the CVAE is to explicitly impose signer independence on the learned latent representations. This is achieved by encouraging the CVAE to learn latent representations whose conditional posterior distribution, given the image and its sign label, is independent of the signer identity. Accordingly, the learned latent representations will preserve as much information as possible about the class (sign), and discard the irrelevant parts that are signer-specific. In addition, the CVAE acts as a teacher model for the classifier, since the distribution over latent representations is used to regularize the hidden representations of the classifier. These hidden representations are then fed into a multilayer perceptron (MLP) for sign recognition. The result is a signer-independent model robust to new test signers.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.3\columnwidth}
        \begin{tikzpicture}[spy using outlines={circle,red,magnification=4,size=1.4cm, connect spies}]
            \node {\includegraphics[interpolate=true, height=0.50\columnwidth]{ChapterFour/1_psl.png}};
            \spy on (0.05,0.23) in node [left] at (1.6,0.9);
        \end{tikzpicture}
    \end{minipage}
    \hspace{0.00mm}
    \begin{minipage}[t]{0.3\columnwidth}
        \begin{tikzpicture}[spy using outlines={circle,red,magnification=4,size=1.4cm, connect spies}]
            \node {\includegraphics[interpolate=true, height=0.50\columnwidth]{ChapterFour/2_psl.png}};
            \spy on (0.16,0.07) in node [left] at (1.6,0.9);
        \end{tikzpicture}
    \end{minipage}
    \hspace{0.00mm}
    \begin{minipage}[t]{0.3\columnwidth}
        \begin{tikzpicture}[spy using outlines={circle,red,magnification=4,size=1.4cm, connect spies}]
            \node {\includegraphics[interpolate=true, height=0.50\columnwidth]{ChapterFour/3_psl.png}};
            \spy on (0.02,0.08) in node [left] at (1.6,0.9);
        \end{tikzpicture}
    \end{minipage}
    \\
    \begin{minipage}[t]{0.3\columnwidth}
        \begin{tikzpicture}[spy using outlines={circle,red,magnification=4,size=1.4cm, connect spies}]
            \node {\includegraphics[interpolate=true, height=0.50\columnwidth]{ChapterFour/4_psl.png}};
            \spy on (0.04,0.05) in node [left] at (1.6,0.9);
        \end{tikzpicture}
    \end{minipage}
    \hspace{0.00mm}
    \begin{minipage}[t]{0.3\columnwidth}
        \begin{tikzpicture}[spy using outlines={circle,red,magnification=4,size=1.4cm, connect spies}]
            \node {\includegraphics[interpolate=true, height=0.50\columnwidth]{ChapterFour/5_psl.png}};
            \spy on (0.001,-0.05) in node [left] at (1.6,0.9);
        \end{tikzpicture}
    \end{minipage}
    \hspace{0.00mm}
    \begin{minipage}[t]{0.3\columnwidth}
        \begin{tikzpicture}[spy using outlines={circle,red,magnification=4,size=1.4cm, connect spies}]
            \node {\includegraphics[interpolate=true, height=0.50\columnwidth]{ChapterFour/6_psl.png}};
            \spy on (0.02,0.17) in node [left] at (1.6,0.9);
        \end{tikzpicture}
    \end{minipage}
    \caption{Illustration of the inter-signer variability using some samples of the presented SI-PSL database. The six signers are performing the sign \textquotedblleft eight\textquotedblright~of the LGP.}
    \label{fig:si_signer_variability}
\end{figure}

The remainder of this section is organized as follows: The proposed signer-independent deep neural network along with the proposed loss function and regularization schemes are fully described in Section \ref{sec:proposed_method}. Section \ref{sec:experiments} reports the experimental evaluation of the proposed methodology, in which a comparison with state-of-the-art and baseline methods is performed. Finally, conclusions and some topics for future work are presented in Section \ref{sec:conclusion}.

\subsection{The DeSIRe model}

The high-level block diagram of the proposed DeSIRe model is depicted in Figure \ref{fig:model_archi}. As it is possible to observe, it is composed by a CVAE and a classifier. In our model, the underlying idea of the CVAE is to learn an invertible mapping to a space where the signer-specific information is disentangled from the discriminative properties of the sign class. The CVAE can be thought as a teacher model for the classifier, as the distribution over latent representations $\rvz$ is used to regularize the hidden representations $\tilde{\vz}$ of the classifier. These hidden representations $\tilde{\vz}$ are then fed into a multilayer perceptron (MLP) for a robust signer-independent SLR.

Specifically, the CVAE consists of an encoder and a decoder network, parameterized by $\vtheta_e$ and $\vtheta_d$, respectively. The purpose of the encoder network is to learn a distribution $q(\rvz \mid \rmX, \ry, \rs; \theta_e)$ which approximates the true posterior distribution of the latent code $\rvz$ given the image $\rmX$, the class label $\ry$ and the signer identity $\rs$. By conditioning the posterior distribution on $\rs$ and $\ry$, we are empowering the encoder by learning a domain and class-dependent transformation. Here, the key idea is to learn latent codes whose conditional posterior distribution is independent of the signer identity, that is $q(\rvz \mid \rmX, \ry, \rs; \theta_e) \equiv q(\rvz \mid \rmX, \ry; \theta_e)$. Equivalently, latent codes are conditionally independent of the signer identity given the image and its class if and only if:
\begin{equation}
    \label{eq:independence}
    q(\rvz \mid \rmX, \ry, \rs=s; \theta_e)~=~q(\rvz \mid \rmX, \ry, \rs=s'; \theta_e),
\end{equation}
for any two distinct signers $s$ and $s'$. In order to promote this signer-independence property, the loss function includes a term that penalizes deviations from this equality. However, if no additional care is taken, this condition would compete with the reconstruction objective, since reconstructing an image implies preserving as much information about the image as possible, including signer-specific information. Therefore, the signer identity is sampled uniformly at random and fed as an additional input to the decoder network. By this mean, the decoder shall provide a disentangled representation of the signer identity which, combined with the signer-invariant latent code $\vz$, will be used to reconstruct the original sample.

Intuitively, as the latent vector $\vz$ is sampled from $q(\rvz \mid \rmX, \ry, \rs; \theta_e)$, the latent representations $\vz$ will preserve as much information as possible about the class (sign), and discard the irrelevant parts that are characteristic of each signer. The loss function is defined in such a manner that it encourages similarity between the latent codes $\vz$ and the hidden representations $\tilde{\vz}$ of the classifier module. The classifier is then trained on these signer-invariant representations for a robust signer-independent SLR. Formally, $h(\cdot; \vtheta_h): \gZ \mapsto [0,1]^{|\gY|}$ represents our task-specific function, parameterized by $\vtheta_h$, that maps from the hidden representation to the predicted sign class $\hat{y}$, and $g(\cdot; \vtheta_g): \gX \mapsto \gZ$ denotes an encoding function, parameterized by $\vtheta_g$, that maps the input images to the corresponding hidden representations.

\subsubsection{Loss function}
\label{sec:loss}
Training the proposed DeSIRe model is achieved by minimizing the following loss function with respect to parameters $\Theta=\{\vtheta_{e},\vtheta_{d},\vtheta_{g},\vtheta_{h}\}$:
\begin{equation}
    L(\Theta) = L_{\text{CVAE}}(\vtheta_{d},\vtheta_{e}) + \lambda_{1} L_{\text{emb}}(\vtheta_{e},\vtheta_{f}) + \lambda_{2} L_{\text{class}}(\vtheta_{f},\vtheta_{g}),
\end{equation}
where $\lambda_{1},\lambda_{2}\geq 0$ are the weights that control the interaction between the loss terms.

The ultimate goal of the CVAE loss, $L_{\text{CVAE}}$, is to explicitly impose signer independence by learning latent representations which are conditionally independent from the signer identity. In this regard, $L_{\text{CVAE}}$ is defined by:
\begin{equation}
    L_{\text{CVAE}}(\theta_{d},\theta_{e}) = L_{\text{rec}}(\theta_{d})~+~\alpha_{1}~L_{\text{prior}}(\theta_{e})~+~\alpha_{2}~L_{\text{signer\_inv}}(\theta_{e}),
\end{equation}
where $\alpha_{1},\alpha_{2}\geq 0$ are hyperparameters that control the relative importance of each loss term. The first two terms, $L_{\text{rec}}$ and $L_{\text{prior}}$, correspond to the loss function of a standard CVAE, containing some special modifications for promoting signer-independence in the latent space. The reconstruction loss $L_{\text{rec}}$ encourages the decoder to learn how to reconstruct the input data $\rmX$. For the decoder, we assume that the conditional likelihood of the data $\rmX$ given the latent code $\rvz$ and the signer identity $\rs$ follows a Gaussian distribution. Accordingly, as explained in \Secref{sec:background_cvae}, the reconstruction loss corresponds to the mean-squared error between a training image and a generated image. Here, however, instead of working with pairs of ground-truth images together with their respective reconstructions, we make a slight modification that further promotes signer-invariant encodings. Let $\mX^{(r)}_{y,s}$ denote the $r$-th image of signer $s$ and sign class $y$. Specifically, we compute the mean-squared error between the $j$-th $d$-dimensional training image $\mX^{(r_{j})}_{y_{j},s_{j}}$ and the generated $d$-dimensional image $\vmu_d(\vz_{i},s_{j}; \theta_{d})$ which is produced by the decoder when fed with the encoding $\vz_{i}$ of the $i$-th training image $\mX^{(r_{i})}_{y_{i},s_{i}}$ and with the signer identity $s_j$ of the $j$-th training image:
\begin{equation}
    \label{eq:loss_rec}
    L_{\text{rec}}(\vtheta_{d}) = \frac{1}{n d}~\sum_{i=1}^{n}||\mX^{(r_{j})}_{y_{j},s_{j}}-\vmu_d(\vz_{i},s_{j}; \theta_{d})||^2_2,
\end{equation}
where $y_{j}=y_{i}$, $\vz_{i}$ is sampled from $q(\rvz_{i} \mid \mX^{(r_{i})}_{y_{i},s_{i}}, y_{i}, s_{i}; \theta_e)$ using the reparameterization trick \plaineqref{eq:reparam_trick}, $s_{j}$ is sampled from a distribution $w(\rs \mid s_{i})$, defined below, and $r_{j}$ is sampled uniformly from the set of available repetitions:
\begin{equation}
    w(\rs \mid s_{i}) =
    \begin{cases}
        1-\rho, \quad \rs=s_{i}, \\
        \frac{\rho}{k-1}, \quad \rs \in \{1,2,\dots,k\} \setminus \lbrace s_{i} \rbrace.
    \end{cases}
\end{equation}
Here, as before, $\{1,2,\dots,k\}$ is the set of signer identities in the training data and $\rho \in [0, 1]$ is a hyperparameter. By sampling the identity $s_{j}$ of the ground-truth image from $w(\rs \mid s_{i})$, decoder will be trained to reconstruct an image of a different subject (but same sign class) than the one that was used to produce the encoding. This will happen in a proportion $\rho$ of the cases. This procedure further discourages the latent codes to preserve signer-specific information and therefore aims to reduce inter-signer variability. On the other hand, by sampling the sign repetition $r_{j}$, the decoder will also be trained to reconstruct a distinct image of the same person and sign class as the image that produced the encoding. Here, the purpose is to gain robustness to intra-signer variability. Although less problematic than the former, this type of variability is also relevant since the same signer does not always repeat the same sign in exactly the same way. Moreover, different image acquisition conditions (e.g.\ background, illumination, distance to the camera, etc.) from one repetition to another also result in intra-signer variability.

The $L_{\text{prior}}$ term corresponds to the KL divergence between the posterior and the prior as commonly used in a standard CVAE:
\begin{align}
    \label{eq:loss_prior}
    L_{\text{prior}}(\vtheta_{e}) &= \frac{1}{nl} \sum_{i=1}^n \KL(q(\rvz_{i} \mid \mX^{(r_{i})}_{y_{i},s_{i}}, y_{i}, s_{i}; \vtheta_e) || \gN(\rvz_{i}; 0, \mI)) \nonumber\\
    &= \frac{1}{2nl} \sum_{i=1}^n \sum_{j=1}^l \left(\mu_{e,i,j}^2 + \sigma_{e,i,j}^2 -1 - \log \sigma_{e,i,j}^2\right),
\end{align}
where $l$ is the dimension of the latent space and $\mu_{e,i,j}$ and $\sigma_{e,i,j}$ denote the $j$-th elements of the vectors $\vmu_{e}(\mX_i, y_i, s_i; \vtheta_e)$ and $\vsigma_{e}(\mX_i, y_i, s_i; \vtheta_e)$, respectively.

An explicit constraint for signer-independence is also introduced in the CVAE loss function. $L_{\text{signer\_inv}}$ encourages the conditional posterior distribution of latent codes $\rvz$, given the image $\rmX$ and its class $\ry$, to be independent of the signer identity $\rs$. This loss is defined as the KL divergence between conditional posterior distributions of $\rvz$, conditioned on the same class but also on different signer identities:
\begin{align}
    \label{eq:loss_signer_inv}
    \mathcal{L}_{\text{signer\_inv}}(\vtheta_{e}) &= \frac{1}{nl} \sum_{i=1}^{n}\KL\left(q(\rvz_{i} \mid \mX^{(r_{i})}_{y_{i},s_{i}}, y_{i}, s_{i}; \vtheta_e) \big| \big| q(\rvz_{k} \mid \mX^{(r_{k})}_{y_{k},s_{k}}, y_{k}, s_{k}; \vtheta_e)\right) \nonumber\\
    &=\frac{1}{2nl}~\sum_{i=1}^{n}\sum_{j=1}^{l} \Biggl(\frac{(\mu_{e,i,j} - \mu_{e,k,j})^2}{\sigma_{e,k,j}^2} + \frac{\sigma_{e,i,j}^2}{\sigma_{e,k,j}^2} -1 + \log \sigma_{e,k,j}^2 - \log \sigma_{e,i,j}^2 \Biggr),
\end{align}
where $y_{k}=y_{i}$ and $s_k$ is sampled uniformly from $\{1,2,\dots,k\} \setminus \lbrace s_i \rbrace$. The second equality follows from the fact that both distributions are Gaussian and so their KL divergence may be computed analytically, as in \eqref{eq:loss_prior}.

The signer-invariant latent representations $\rvz$ learned by the CVAE are then used to regularize the hidden representations $\vh$ of the classifier. Such regularization is promoted by the $L_{\text{emb}}$ loss term, which encourages the latent representations of the CVAE and the classifier to be as similar as possible. Following this idea, the embedding loss $L_{\text{emb}}$ is defined to minimize the expected mean-squared error between $\rvz$ and $\vh$, that is:
\begin{equation}
    \label{eq:emb_loss}
    L_{\text{emb}}(\theta_e, \theta_f)~=~\frac{1}{NL}~\sum_{i=1}^{N}\E_{\rvz_{i} \sim q(\rvz_{i} \mid \mX^{(r_{i})}_{y_{i},s_{i}}, y_{i}, s_{i}; \theta_e)}||\rvz_i-\vh_i||^2.
\end{equation}
In practice, we replace \eqref{eq:emb_loss} by its Monte Carlo approximation with one sample, which yields:
\begin{equation}
    L_{\text{emb}}(\theta_e, \theta_f)~=~\frac{1}{NL}~\sum_{i=1}^{N}||\vz_i-\vh_i||^2,
\end{equation}
where $\vz_i$ is sampled from $q(\rvz_{i} \mid \mX^{(r_{i})}_{y_{i},s_{i}}, y_{i}, s_{i}; \theta_e)$, again using the reparameterization trick \plaineqref{eq:reparam_trick}. This approximation has an extra regularizing effect on the classifier network, by introducing some stochastic noise in its training routine.

Finally, the classification loss, $L_{class}$, trains the model to predict the output sign labels and corresponds to the categorical cross-entropy, defined by:
\begin{equation}
    \label{eq:desire_loss_class}
    L_{\text{class}}(\vtheta_{g},\vtheta_{h}) = -\frac{1}{n} \sum_{i=1}^{n}\log p(y_{i} \mid \mX^{(r_{i})}_{y_{i}, s_{i}};\vtheta_{g},\vtheta_{h}),
\end{equation}
where $p(y \mid \mX; \vtheta_{g},\vtheta_{h})$ is the predicted probability that a given image $\mX$ belongs to its ground-truth class $y$, according to the current classifier parameters $\vtheta_{g}$ and $\vtheta_{h}$.