% Chapter Template

% Main chapter title
%\chapter[toc version]{doc version}
\chapter{Domain generalization}

% Short version of the title for the header
%\chaptermark{version for header}

% Chapter Label
% For referencing this chapter elsewhere, use \ref{ChapterTemplate}
\label{chp:domain_generalization}

% Write text in here
% Use \subsection and \subsubsection to organize text

\begin{tcolorbox}
	\small{
		Some parts of this chapter were originally published in or adapted from:
		\begin{itemize}
			\item[] \cite{DeSIRe} \bibentry{DeSIRe} (presented in \Secref{sec:desire})
			\item[] \cite{AdvSInvConf} \bibentry{AdvSInvConf} (presented in \Secref{sec:adv_signer_inv})
			\item[] \cite{AdvSInvJournal} \bibentry{AdvSInvJournal} (idem)
			\item[] \cite{AdvInvAttack} \bibentry{AdvInvAttack} (\Secref{sec:adv_inv_attack})
		\end{itemize}

		The first two authors contributed equally in \cite{DeSIRe} and \cite{AdvSInvConf}. Both conceived the models and designed and conducted the experiments, with the supervision of Rebelo and Cardoso. The work in \cite{AdvSInvJournal} extends \cite{AdvSInvConf} by including a more exhaustive experimental evaluation. In \cite{AdvInvAttack}, Diogo Pernes contributed on the development of the proposed methodology, together with the first two authors, who formalized the problem and conducted all experiments. Cardoso supervised the work.
	}
\end{tcolorbox}

\section{Introduction}
\label{sec:chp4_intro}
In Chapters \ref{chp:networked_data_streams} and \ref{chp:domain_adaptation}, the target entities/domains were known at training time. In Chapter \ref{chp:networked_data_streams}, we exploited the correlations between different but related entities to augment the amount of data available for each of those and hence improve the in-distribution generalization. Chapter \ref{chp:domain_adaptation} was dedicated to the problem of domain adaptation, whose purpose is to improve the out-of-distribution (OOD) generalization in a specific target domain for which no labeled data is available.

In this chapter, we shall continue focusing on OOD generalization. However, now, the target domain is unknown and, therefore, no data from this domain is available at training time, neither labeled nor unlabeled. The purpose, then, is to use labeled data from multiple source domains to build a discriminative model that generalizes well to unknown OOD target domains -- a problem known as \emph{domain generalization} (\citet{Blanchard2011, Muandet2013}). Our main assumption to accomplish this goal is that the set of features that are relevant for the learning task are domain-invariant. Formally, we assume that, for each domain $\gD$, there exists a bijection $b_\gD: \gX \mapsto \gZ \times \gW$, where $\gZ$ is the domain-invariant space of features used for classification and $\gW$ are domain-specific auxiliary features carrying no relevant signal for the considered learning task. Thus, for $(\rz, \rw) \triangleq b_\gD(\rvx)$, we assume that $p_\gD(\ry \mid \rx) = p(\ry \mid \rz)$, i.e.\ the optimal classifier for any domain $\gD$ can be reconstructed from features in $\gZ$ and a domain-invariant classifier $p(\ry \mid \rz)$.  This formulation is closely related to the covariate shift assumption for domain adaptation, described in \secref{sec:cov_shift_sota}.

A computer vision application where this problem is particularly relevant is sign language recognition (SLR). Large inter-signer variability in the manual signing process of sign languages is one of the challenges associated with this task. Due to this issue, models trained on data from a given set of signers often fail to generalize well when tested on previously unseen signers. Since, ideally, an SLR system should be able to recognize the gestures of any signer, this problem should be tackled with domain generalization (DG) techniques. For this reason, SLR will be the main application considered in this chapter. Nonetheless, we will also show that the same principles can be applied successfully to develop a fingerprint presentation attack detection method that exhibits robust performance on detecting unseen attacks.

The remainder of this chapter is organized as follows: i) we start by presenting the state of the art for DG (\secref{sec:dg_sota}); ii) we present a novel adversarial-based approach for DG in the context of SLR (\secref{sec:adv_signer_inv}); iii) we show how this methodology can be successfully adapted to address the problem of fingerprint presentation attack detection (\secref{sec:adv_fingerprint}); iv) we present a novel reconstruction-based algorithm for DG (\secref{sec:desire}).

\section{State of the art}
\label{sec:dg_sota}
\citet{Zhou2021} divide the algorithms for domain generalization as heterogeneous and homogeneous, depending on whether the label space varies (heterogeneous DG) or not (homogeneous DG). The former case is also known as \emph{zero-shot} domain generalization and its goal is in general to learn a feature representation that can be used in the target domain to recognize new classes. The latter, which will be the focus of this chapter, is closely related to domain adaptation, so there is a significant intersection between the two. \citet{Albuquerque2019} presented an upper bound for the generalization error that is essentially an upper bound for multi-source domain adaptation, similar to the bound by \citet{Zhao2018} (Theorem~\ref{thm:da_bound_multi_source}) and to our own (Theorem~\ref{thm:target_risk_bound}).

The theoretical proximity between the two problems motivates the existence of similar algorithms to tackle them. As a matter of fact, many algorithms for DG follow the paradigm of domain alignment, which we have discussed extensively in the context of DA. \citet{Li2018} use an adversarial autoencoder and maximum mean discrepancy over its latent space to learn domain-invariant features. \citet{Ghifary2015} address the same problem through a multi-output autoencoder, which is trained to transform samples from one domain into samples from the remaining domains with the same label. \citet{Motiian2017} proposed a unified framework to address the problems of domain adaptation and generalization. They use a contrastive $\normltwo$-loss in the latent space that pushes together samples from different domains and the same class while pulling apart samples from different classes. Several other approaches extend the idea of domain adversarial networks (\citet{Ganin2015}) to the problem of domain generalization, by using domain classifiers and minimax training to learn domain-invariant features. Some of those use a single multi-class classifier to classify samples into one of $k$ source domains (e.g.\ \citet{Aslani2020, Matsuura2020}) and others employ $k$ binary domain discriminators trained in a one-vs-all manner (e.g.\ \citet{Shao2019, YaLi2018}).

Ensemble learning has also been widely applied to the problem of domain generalization. \citet{Zheng2014} train support vector machines (SVMs) with a single positive example and a few negative examples (known as \emph{exemplar-SVMs}) and use the most confident classifiers in an ensemble to make the final prediction. More recent approaches replace the SVM with deep neural networks and build ensembles of domain-specific networks, either by weighting all the predictions equally (e.g.\ \citet{Innocente2018, Zhou2020}) or by using the output of a domain classifier as sample-dependent ensemble weights (\citet{Wang2020}).

Self-supervised learning (SSL) techniques are becoming increasingly popular in machine learning and have also been applied to the problem of DG. SSL refers to the task of learning from free labels, i.e.\ it consists of standard supervised learning for tasks where the labels can be extracted automatically from the data, without the need of manual annotation. Examples of SSL tasks are predicting the next word in a sentence, image colorization (\citet{Zhang2016}), predicting the relative position of image patches (\citet{Doersch2015}), predicting if a video is being played forward or backward (\citet{Wei2018}), etc. The idea motivating SSL is that the features learned by pretraining the model on self-supervised tasks provide good initializations for the model, which can then be finetuned for the desired task using a smaller amount of annotated data. In the scope of DG, SSL provides useful features regardless of the target task, reducing the overfitting to domain-specific biases (\citet{Zhou2021}). This idea was followed by \citet{Carlucci2019} and \citet{Wang2020}, who trained a network to solve the Jigsaw puzzle (i.e.\ to place nine shuffled image patches back into their correct positions) as an auxiliary task to enhance domain generalization.

For a more complete review of DG theory and algorithms, please see \citet{Wang2021} and \citet{Zhou2021}.
