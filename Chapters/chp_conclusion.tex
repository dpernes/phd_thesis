% Chapter Template

% Main chapter title
%\chapter[toc version]{doc version}
\chapter{Conclusion}

% Short version of the title for the header
%\chaptermark{version for header}

% Chapter Label
% For referencing this chapter elsewhere, use \ref{ChapterTemplate}
\label{chp:conclusion}

% Write text in here
% Use \subsection and \subsubsection to organize text

\section{Summary of contributions}
This thesis addressed the problem of multi-entity and multi-domain learning under various settings, motivated by a wide variety of applications and using multiple data modalities, from Wi-Fi data streams to video. Its outcomes have been published in international conferences and journals and, in many cases, benefited from joint efforts with other members of our research group at INESC TEC. These collaborations enhanced the focus of this work on more application-oriented topics which further demonstrate the practical relevance of multi-entity learning. Our contributions throughout this document are summarized as follows:
\begin{itemize}
    \item We showed how the self-organizing hidden Markov model map (\Secref{sec:sohmmm}) and its learning algorithm could be generalized to other types of data. In particular, we demonstrated the effectiveness of this approach on the problem of anomaly detection on Wi-Fi networks with multiple access points.
    \item We presented the sparse mixture of hidden Markov models (\Secref{sec:spamhmm}) as a more flexible, general, and expressive model to learn the generative distribution of multi-entity data streams. Unlike SOHMMM, SpaMHMM learns a many-to-many relationship between network entities and atoms in the dictionary. We have also shown how to incorporate prior knowledge about inter-entity similarity into the learning algorithm. The model was experimentally validated in different applications, confirming its effectiveness and versatility.
    \item We discussed how the SpaMHMM framework could be generalized (\Secref{sec:generalizing_spamhmm}), showing that this model is a particular realization of a broad family that can be used to learn the generative distribution of multi-entity streams. Just like SpaMHMM, this meta-model comprises an entity-dependent latent distribution and a shared conditional distribution of observations given latent codes.
    \item We then started addressing out-of-distribution generalization in the context of multi-source domain adaptation. A deep neural network for multi-source domain adaptation and another one for object counting in videos were combined in different and meaningful ways (\Secref{sec:da_sensors}). Their performance was compared in two different tasks related to counting objects in videos captured by multiple cameras.
    \item Next, we proposed a novel model for unsupervised multi-source domain adaptation (\Secref{sec:modafm}). This model overcomes some of the major limitations of previous state-of-the-art approaches, namely the overly pessimistic weighting of source domains and the curse of domain-invariant representations. The proposed approach outperforms the state of the art in most benchmark datasets for multi-source domain adaptation.
    \item Domain generalization, which is yet another OOD generalization problem, was addressed in the next chapter. We proposed an adversarial-network-based DG algorithm that differs from other existing methods in the formulation of the adversarial objective. Our version not only has an intuitive explanation, but also provides a global minimum and bounded gradients, which is not the case with the standard minimax approach. The algorithm was initially presented in the context of signer-independent sign language recognition (\Secref{sec:adv_signer_inv}) and later adapted to the problem of iris presentation attack detection (\Secref{sec:adv_iris_attack}).
    \item Our final contribution is DeSIRe, a DG model specially tailored to the problem of signer-independent SLR (\Secref{sec:desire}). This model employs a variational autoencoder and explicit signer-independence constraints in its latent space to learn latent codes that are truly signer-invariant and yet highly discriminative for the sign classification task. Its performance outperforms that of our previous model for SLR and all baselines we have compared it to.
\end{itemize}

\section{Final remarks and directions for future work}
This thesis addressed three major subtopics in the context of multi-entity and multi-domain learning, each one motivating its own chapter in this thesis. Although the contributions presented here are hopefully relevant, there are several problems left unanswered or explored only superficially.

In the context of networked data streams, we have summarized how our proposed models could be generalized to more challenging learning scenarios like the online and distributed setting and how the HMM could be replaced by more expressive models, like deep recurrent neural networks. However, those ideas were not validated or implemented in practice. In particular, it would be interesting to observe if the approach is  capable of scaling to higher-dimensional data streams.

We have also only scratched the surface of domain adaptation for data streams and video. Video data is still a challenging modality to learn from, in part due to the large computational overhead it requires. In the context of domain adaptation, it is still unclear how to exploit different temporal dynamics from different sources to improve the  algorithm performance. Nonetheless, domain adaptation for non-sequential data is also not a solved problem. The model we have proposed is effective in mitigating the curse of domain-invariant representations for image data, where several and diverse label-preserving transformations are available as part of any computer vision toolbox. However, the procedure does not generalize straightforwardly to non-visual data. Another somewhat unaddressed problem is domain selection when the number of available source domains is very large. Although our model adjusts the source domain weights automatically and dynamically, it was not validated with more than five source domains. It is a known fact that adding more source domains can hurt the model performance on the target when the domain shift is large. Moreover, one can think of many practical applications where the number of source domains is in the order of dozens, hundreds, or even more, and therefore a wise selection of the source data to be used for a particular target domain is necessary.

Finally, in the context of domain generalization and, in particular, of automatic sign language recognition, the continuous setting has not been addressed. It is clear that an a automatic SLR system in the real world has to deal with video sequences, rather than with individual frames. Nevertheless, the approaches we have proposed to learn signer-invariant representations could be applied on a per-frame basis and then integrated into the automatic SLR pipeline, whose development is a problem beyond the scope of this thesis.